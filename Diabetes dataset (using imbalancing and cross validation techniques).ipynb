{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75bf273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64f722e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(r\"C:\\Users\\Acer\\Desktop\\Dataset\\diabetes_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1bbf1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0              6      148             72             35        0  33.6   \n",
       "1              1       85             66             29        0  26.6   \n",
       "2              8      183             64              0        0  23.3   \n",
       "3              1       89             66             23       94  28.1   \n",
       "4              0      137             40             35      168  43.1   \n",
       "..           ...      ...            ...            ...      ...   ...   \n",
       "763           10      101             76             48      180  32.9   \n",
       "764            2      122             70             27        0  36.8   \n",
       "765            5      121             72             23      112  26.2   \n",
       "766            1      126             60              0        0  30.1   \n",
       "767            1       93             70             31        0  30.4   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                       0.627   50        1  \n",
       "1                       0.351   31        0  \n",
       "2                       0.672   32        1  \n",
       "3                       0.167   21        0  \n",
       "4                       2.288   33        1  \n",
       "..                        ...  ...      ...  \n",
       "763                     0.171   63        0  \n",
       "764                     0.340   27        0  \n",
       "765                     0.245   30        0  \n",
       "766                     0.349   47        1  \n",
       "767                     0.315   23        0  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e718f4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaf47179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pregnancies                 0\n",
       "Glucose                     0\n",
       "BloodPressure               0\n",
       "SkinThickness               0\n",
       "Insulin                     0\n",
       "BMI                         0\n",
       "DiabetesPedigreeFunction    0\n",
       "Age                         0\n",
       "Outcome                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30ccc7f",
   "metadata": {},
   "source": [
    "# Check the data is imbalancing or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e2d44e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    500\n",
       "1    268\n",
       "Name: Outcome, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Outcome\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61ebe130",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data.iloc[:,:-1].values\n",
    "y=data.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d775c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) If you have imbalancing in your model please apply oversampling or SMOTE technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1958ba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "s=SMOTE()\n",
    "x_data,y_data=s.fit_resample(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab8eab0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Original data : Counter({0: 500, 1: 268})\n",
      "The Artificial data : Counter({1: 500, 0: 500})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"The Original data :\",Counter(y))\n",
    "print(\"The Artificial data :\",Counter(y_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33167df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) After solving the imbalancing please apply k-fold cross validation technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ebeb0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=2, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf=KFold(n_splits=5,random_state=2,shuffle=True)\n",
    "kf.get_n_splits(x_data)\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c7ee340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN :  [  0   2   3   4   5   6   8   9  10  11  12  13  14  16  17  18  19  21\n",
      "  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  38  39  40\n",
      "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
      "  59  60  61  62  63  64  66  69  72  73  75  76  78  79  80  81  82  83\n",
      "  85  86  87  88  90  91  92  93  94  95  96  97  98 100 102 103 104 105\n",
      " 106 107 108 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124\n",
      " 125 126 127 129 130 131 132 133 134 135 136 138 139 140 144 145 147 148\n",
      " 149 150 151 152 153 154 155 156 157 159 161 162 163 165 166 167 168 170\n",
      " 171 173 174 175 176 177 178 180 181 184 186 187 188 189 190 191 194 195\n",
      " 196 198 200 201 202 203 204 206 207 208 210 211 214 215 216 217 218 219\n",
      " 220 221 222 224 225 227 228 229 233 234 235 237 238 240 241 242 243 244\n",
      " 245 246 247 248 249 252 253 254 255 256 257 259 260 261 262 263 264 265\n",
      " 266 268 269 270 271 273 274 275 276 277 278 281 282 283 285 287 289 290\n",
      " 292 294 296 297 298 299 300 301 302 303 304 305 306 307 308 309 311 313\n",
      " 314 315 316 317 318 319 320 321 323 324 325 326 327 328 329 331 332 334\n",
      " 335 336 337 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353\n",
      " 354 356 357 358 359 360 361 362 364 365 366 367 368 369 371 372 373 374\n",
      " 377 378 380 381 382 383 384 385 386 387 389 390 392 393 394 395 399 400\n",
      " 401 402 403 404 405 406 407 408 410 413 415 416 417 419 420 421 422 424\n",
      " 425 427 428 429 430 433 434 435 436 437 438 439 440 441 442 443 446 447\n",
      " 448 449 450 452 453 454 456 457 458 459 460 461 466 467 468 469 470 471\n",
      " 472 473 474 475 476 477 478 479 482 483 485 487 488 489 490 491 492 493\n",
      " 494 495 496 497 498 499 500 501 502 503 505 506 507 509 510 511 512 513\n",
      " 514 516 517 519 520 521 523 524 526 527 528 529 530 531 532 533 534 536\n",
      " 537 538 539 540 541 542 544 545 547 548 550 551 552 554 555 558 559 560\n",
      " 561 562 563 564 565 566 567 568 569 570 571 572 573 574 579 581 582 583\n",
      " 584 585 586 587 588 590 591 592 593 594 595 597 598 600 601 602 603 604\n",
      " 605 606 607 608 609 610 611 613 617 618 619 620 622 623 624 625 626 627\n",
      " 628 629 630 631 632 633 634 635 636 637 638 641 642 643 644 645 647 648\n",
      " 649 651 654 655 656 657 659 660 661 662 663 665 666 667 668 669 670 671\n",
      " 672 673 674 675 676 677 679 680 681 684 686 688 689 690 692 693 695 696\n",
      " 697 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 717\n",
      " 718 719 720 721 722 723 724 725 728 729 730 731 734 735 736 738 739 740\n",
      " 741 743 744 745 746 747 749 750 751 752 753 754 755 756 758 759 760 761\n",
      " 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 780\n",
      " 781 782 783 784 785 786 787 789 790 791 792 793 795 796 799 800 801 803\n",
      " 804 805 806 807 808 813 814 816 817 818 819 820 822 823 825 826 831 833\n",
      " 835 836 837 838 841 842 843 844 845 847 849 850 851 852 853 854 855 857\n",
      " 858 859 861 862 864 865 866 867 868 870 872 873 874 875 876 877 878 879\n",
      " 882 883 884 885 888 889 890 891 892 893 894 895 896 898 899 900 902 903\n",
      " 904 905 907 909 910 911 914 915 916 917 918 919 920 921 922 923 926 928\n",
      " 932 933 934 936 938 939 940 941 942 943 944 945 946 947 948 949 950 952\n",
      " 953 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971\n",
      " 972 973 975 976 977 978 979 980 981 982 983 985 986 987 988 989 990 991\n",
      " 992 993 994 995 996 997 998 999] TEST :  [  1   7  15  20  37  65  67  68  70  71  74  77  84  89  99 101 109 128\n",
      " 137 141 142 143 146 158 160 164 169 172 179 182 183 185 192 193 197 199\n",
      " 205 209 212 213 223 226 230 231 232 236 239 250 251 258 267 272 279 280\n",
      " 284 286 288 291 293 295 310 312 322 330 333 338 355 363 370 375 376 379\n",
      " 388 391 396 397 398 409 411 412 414 418 423 426 431 432 444 445 451 455\n",
      " 462 463 464 465 480 481 484 486 504 508 515 518 522 525 535 543 546 549\n",
      " 553 556 557 575 576 577 578 580 589 596 599 612 614 615 616 621 639 640\n",
      " 646 650 652 653 658 664 678 682 683 685 687 691 694 698 715 716 726 727\n",
      " 732 733 737 742 748 757 779 788 794 797 798 802 809 810 811 812 815 821\n",
      " 824 827 828 829 830 832 834 839 840 846 848 856 860 863 869 871 880 881\n",
      " 886 887 897 901 906 908 912 913 924 925 927 929 930 931 935 937 951 954\n",
      " 974 984]\n",
      "TRAIN :  [  0   1   3   5   6   7   8   9  11  14  15  17  19  20  21  23  24  25\n",
      "  26  27  28  31  33  34  35  36  37  38  39  41  43  44  45  46  47  48\n",
      "  49  50  51  52  53  54  56  57  59  60  61  63  64  65  66  67  68  69\n",
      "  70  71  73  74  75  76  77  78  79  81  82  83  84  85  86  87  88  89\n",
      "  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 109 110 112\n",
      " 113 114 115 117 118 120 121 122 123 124 125 127 128 130 131 132 133 134\n",
      " 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 153\n",
      " 154 155 158 159 160 161 162 164 165 166 167 168 169 170 171 172 173 179\n",
      " 180 182 183 184 185 186 187 189 190 191 192 193 194 195 196 197 199 201\n",
      " 205 206 207 208 209 210 211 212 213 214 215 217 218 219 220 221 222 223\n",
      " 224 225 226 228 229 230 231 232 233 234 236 237 238 239 240 242 243 244\n",
      " 245 246 247 249 250 251 252 253 254 255 256 258 259 260 261 262 263 264\n",
      " 265 266 267 268 269 270 271 272 274 276 277 278 279 280 281 282 283 284\n",
      " 285 286 287 288 289 290 291 292 293 295 296 297 298 299 300 301 302 303\n",
      " 306 307 310 312 313 314 316 319 320 321 322 323 324 325 326 328 329 330\n",
      " 331 332 333 336 337 338 339 341 342 348 349 350 351 352 353 354 355 357\n",
      " 358 359 360 361 362 363 364 366 367 368 369 370 372 373 375 376 377 378\n",
      " 379 380 381 383 385 386 387 388 389 390 391 392 393 394 395 396 397 398\n",
      " 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 418 420\n",
      " 421 422 423 424 425 426 428 430 431 432 433 434 435 442 443 444 445 446\n",
      " 447 448 449 450 451 454 455 456 461 462 463 464 465 466 467 468 469 470\n",
      " 472 474 475 477 478 479 480 481 482 483 484 485 486 487 489 491 493 494\n",
      " 495 497 498 499 500 501 503 504 505 506 507 508 509 511 512 514 515 516\n",
      " 517 518 520 521 522 523 525 527 528 529 531 533 534 535 536 537 538 539\n",
      " 541 543 544 545 546 547 548 549 550 551 553 554 555 556 557 559 560 561\n",
      " 562 563 564 565 566 568 569 570 571 574 575 576 577 578 579 580 581 582\n",
      " 583 584 585 586 587 588 589 590 591 592 594 595 596 597 598 599 601 602\n",
      " 603 604 605 606 607 608 609 610 611 612 613 614 615 616 618 619 621 622\n",
      " 623 624 625 626 627 628 629 630 631 632 634 635 636 638 639 640 643 644\n",
      " 645 646 647 648 649 650 651 652 653 654 655 656 657 658 661 662 663 664\n",
      " 665 666 667 668 670 671 672 674 676 678 679 680 681 682 683 684 685 687\n",
      " 689 690 691 693 694 695 697 698 703 704 707 708 709 712 713 714 715 716\n",
      " 718 720 721 722 723 724 725 726 727 729 730 731 732 733 734 735 736 737\n",
      " 738 741 742 743 745 746 747 748 750 751 754 755 757 758 759 760 761 762\n",
      " 763 764 765 766 767 768 769 770 772 773 774 775 776 778 779 780 782 784\n",
      " 785 787 788 789 790 792 793 794 796 797 798 799 800 801 802 804 805 806\n",
      " 807 808 809 810 811 812 813 814 815 816 817 820 821 823 824 827 828 829\n",
      " 830 831 832 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848\n",
      " 849 850 851 852 853 855 856 857 858 860 861 863 864 865 866 869 870 871\n",
      " 872 873 875 876 879 880 881 882 884 885 886 887 888 891 892 893 896 897\n",
      " 900 901 902 903 904 905 906 907 908 910 911 912 913 914 915 916 917 918\n",
      " 919 921 922 924 925 926 927 928 929 930 931 932 934 935 936 937 938 939\n",
      " 940 942 943 945 947 949 950 951 953 954 956 958 959 960 961 962 964 965\n",
      " 966 968 970 971 972 974 975 977 978 980 981 983 984 985 986 987 988 990\n",
      " 991 992 993 994 995 997 998 999] TEST :  [  2   4  10  12  13  16  18  22  29  30  32  40  42  55  58  62  72  80\n",
      "  90  91 107 108 111 116 119 126 129 152 156 157 163 174 175 176 177 178\n",
      " 181 188 198 200 202 203 204 216 227 235 241 248 257 273 275 294 304 305\n",
      " 308 309 311 315 317 318 327 334 335 340 343 344 345 346 347 356 365 371\n",
      " 374 382 384 399 400 417 419 427 429 436 437 438 439 440 441 452 453 457\n",
      " 458 459 460 471 473 476 488 490 492 496 502 510 513 519 524 526 530 532\n",
      " 540 542 552 558 567 572 573 593 600 617 620 633 637 641 642 659 660 669\n",
      " 673 675 677 686 688 692 696 699 700 701 702 705 706 710 711 717 719 728\n",
      " 739 740 744 749 752 753 756 771 777 781 783 786 791 795 803 818 819 822\n",
      " 825 826 833 854 859 862 867 868 874 877 878 883 889 890 894 895 898 899\n",
      " 909 920 923 933 941 944 946 948 952 955 957 963 967 969 973 976 979 982\n",
      " 989 996]\n",
      "TRAIN :  [  0   1   2   4   6   7   8   9  10  11  12  13  15  16  17  18  19  20\n",
      "  21  22  26  27  29  30  31  32  33  34  36  37  39  40  42  43  44  45\n",
      "  46  47  49  50  51  55  56  57  58  59  61  62  63  64  65  67  68  70\n",
      "  71  72  74  75  77  79  80  81  82  83  84  85  86  87  88  89  90  91\n",
      "  92  93  95  96  97  98  99 101 102 104 105 106 107 108 109 111 115 116\n",
      " 119 121 122 124 125 126 127 128 129 132 133 135 137 138 140 141 142 143\n",
      " 144 145 146 147 148 149 151 152 153 156 157 158 160 162 163 164 168 169\n",
      " 170 172 174 175 176 177 178 179 181 182 183 184 185 186 187 188 189 190\n",
      " 191 192 193 195 196 197 198 199 200 201 202 203 204 205 207 208 209 210\n",
      " 211 212 213 215 216 218 219 220 223 224 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 241 243 245 248 250 251 252 253 254 255 256 257\n",
      " 258 259 260 261 263 264 267 269 270 271 272 273 274 275 277 278 279 280\n",
      " 282 283 284 286 287 288 290 291 293 294 295 296 298 299 301 302 303 304\n",
      " 305 306 308 309 310 311 312 313 315 316 317 318 319 320 322 323 324 325\n",
      " 326 327 330 332 333 334 335 336 337 338 339 340 341 343 344 345 346 347\n",
      " 348 349 350 351 355 356 357 358 359 360 361 362 363 364 365 366 367 368\n",
      " 370 371 374 375 376 379 380 382 383 384 385 388 389 390 391 392 395 396\n",
      " 397 398 399 400 401 403 404 405 406 408 409 410 411 412 413 414 415 417\n",
      " 418 419 420 421 422 423 424 426 427 428 429 430 431 432 433 434 435 436\n",
      " 437 438 439 440 441 442 444 445 446 448 449 450 451 452 453 454 455 457\n",
      " 458 459 460 461 462 463 464 465 466 469 471 473 474 476 477 478 480 481\n",
      " 482 483 484 485 486 487 488 490 491 492 493 494 495 496 497 498 500 501\n",
      " 502 504 506 507 508 509 510 513 515 516 517 518 519 522 523 524 525 526\n",
      " 527 528 530 531 532 533 534 535 538 539 540 541 542 543 545 546 547 548\n",
      " 549 550 552 553 554 555 556 557 558 560 561 562 564 566 567 569 570 572\n",
      " 573 574 575 576 577 578 580 582 583 584 585 586 587 588 589 590 591 592\n",
      " 593 594 596 598 599 600 602 603 604 605 607 609 610 611 612 613 614 615\n",
      " 616 617 619 620 621 622 623 625 627 628 629 630 631 633 634 635 637 639\n",
      " 640 641 642 646 647 648 649 650 651 652 653 655 657 658 659 660 661 664\n",
      " 665 667 668 669 670 673 674 675 676 677 678 679 680 682 683 685 686 687\n",
      " 688 689 691 692 693 694 695 696 697 698 699 700 701 702 703 705 706 708\n",
      " 709 710 711 713 714 715 716 717 718 719 720 721 722 723 725 726 727 728\n",
      " 729 730 732 733 735 736 737 739 740 741 742 744 745 746 748 749 750 751\n",
      " 752 753 754 756 757 758 759 760 761 762 764 766 767 769 770 771 772 774\n",
      " 775 776 777 778 779 780 781 782 783 784 785 786 788 789 790 791 792 793\n",
      " 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811\n",
      " 812 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830\n",
      " 831 832 833 834 836 837 838 839 840 841 842 843 845 846 847 848 849 850\n",
      " 852 853 854 855 856 859 860 861 862 863 866 867 868 869 870 871 873 874\n",
      " 875 877 878 880 881 883 885 886 887 888 889 890 892 893 894 895 896 897\n",
      " 898 899 900 901 902 903 904 906 908 909 910 912 913 914 917 918 919 920\n",
      " 921 922 923 924 925 926 927 928 929 930 931 933 934 935 937 938 939 941\n",
      " 942 943 944 945 946 947 948 950 951 952 953 954 955 956 957 958 961 963\n",
      " 966 967 969 970 971 972 973 974 975 976 979 980 981 982 983 984 985 987\n",
      " 988 989 992 993 994 995 996 997] TEST :  [  3   5  14  23  24  25  28  35  38  41  48  52  53  54  60  66  69  73\n",
      "  76  78  94 100 103 110 112 113 114 117 118 120 123 130 131 134 136 139\n",
      " 150 154 155 159 161 165 166 167 171 173 180 194 206 214 217 221 222 225\n",
      " 240 242 244 246 247 249 262 265 266 268 276 281 285 289 292 297 300 307\n",
      " 314 321 328 329 331 342 352 353 354 369 372 373 377 378 381 386 387 393\n",
      " 394 402 407 416 425 443 447 456 467 468 470 472 475 479 489 499 503 505\n",
      " 511 512 514 520 521 529 536 537 544 551 559 563 565 568 571 579 581 595\n",
      " 597 601 606 608 618 624 626 632 636 638 643 644 645 654 656 662 663 666\n",
      " 671 672 681 684 690 704 707 712 724 731 734 738 743 747 755 763 765 768\n",
      " 773 787 813 835 844 851 857 858 864 865 872 876 879 882 884 891 905 907\n",
      " 911 915 916 932 936 940 949 959 960 962 964 965 968 977 978 986 990 991\n",
      " 998 999]\n",
      "TRAIN :  [  0   1   2   3   4   5   7   8  10  12  13  14  15  16  18  19  20  21\n",
      "  22  23  24  25  26  28  29  30  31  32  34  35  37  38  40  41  42  43\n",
      "  46  47  48  49  50  51  52  53  54  55  56  58  59  60  61  62  63  65\n",
      "  66  67  68  69  70  71  72  73  74  75  76  77  78  80  81  83  84  86\n",
      "  88  89  90  91  94  96  97  99 100 101 102 103 107 108 109 110 111 112\n",
      " 113 114 116 117 118 119 120 121 122 123 124 125 126 128 129 130 131 132\n",
      " 134 135 136 137 138 139 141 142 143 146 148 150 152 154 155 156 157 158\n",
      " 159 160 161 163 164 165 166 167 168 169 171 172 173 174 175 176 177 178\n",
      " 179 180 181 182 183 185 187 188 192 193 194 195 197 198 199 200 201 202\n",
      " 203 204 205 206 207 209 211 212 213 214 216 217 218 219 221 222 223 224\n",
      " 225 226 227 230 231 232 235 236 238 239 240 241 242 244 245 246 247 248\n",
      " 249 250 251 255 257 258 262 263 265 266 267 268 271 272 273 275 276 279\n",
      " 280 281 284 285 286 287 288 289 291 292 293 294 295 296 297 299 300 302\n",
      " 304 305 307 308 309 310 311 312 314 315 316 317 318 319 321 322 323 324\n",
      " 325 326 327 328 329 330 331 333 334 335 336 338 340 342 343 344 345 346\n",
      " 347 348 351 352 353 354 355 356 359 360 361 363 364 365 366 367 369 370\n",
      " 371 372 373 374 375 376 377 378 379 381 382 384 385 386 387 388 391 392\n",
      " 393 394 396 397 398 399 400 401 402 404 405 406 407 409 411 412 414 416\n",
      " 417 418 419 423 424 425 426 427 429 431 432 433 434 436 437 438 439 440\n",
      " 441 442 443 444 445 446 447 450 451 452 453 454 455 456 457 458 459 460\n",
      " 461 462 463 464 465 466 467 468 470 471 472 473 474 475 476 477 478 479\n",
      " 480 481 482 483 484 485 486 487 488 489 490 491 492 493 496 498 499 502\n",
      " 503 504 505 506 508 509 510 511 512 513 514 515 518 519 520 521 522 524\n",
      " 525 526 527 528 529 530 532 534 535 536 537 538 539 540 542 543 544 545\n",
      " 546 549 550 551 552 553 555 556 557 558 559 561 562 563 564 565 567 568\n",
      " 570 571 572 573 574 575 576 577 578 579 580 581 584 585 587 588 589 590\n",
      " 592 593 595 596 597 599 600 601 604 606 607 608 609 612 614 615 616 617\n",
      " 618 619 620 621 622 624 626 628 632 633 635 636 637 638 639 640 641 642\n",
      " 643 644 645 646 647 648 649 650 651 652 653 654 655 656 658 659 660 661\n",
      " 662 663 664 666 669 670 671 672 673 674 675 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 694 696 697 698 699 700 701 702 703\n",
      " 704 705 706 707 708 709 710 711 712 714 715 716 717 719 720 724 725 726\n",
      " 727 728 730 731 732 733 734 736 737 738 739 740 741 742 743 744 745 746\n",
      " 747 748 749 750 751 752 753 755 756 757 758 759 760 761 762 763 764 765\n",
      " 766 767 768 769 771 772 773 777 779 781 783 784 785 786 787 788 791 794\n",
      " 795 797 798 800 802 803 804 805 807 808 809 810 811 812 813 815 817 818\n",
      " 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 838\n",
      " 839 840 841 844 846 847 848 849 851 854 855 856 857 858 859 860 862 863\n",
      " 864 865 867 868 869 871 872 873 874 875 876 877 878 879 880 881 882 883\n",
      " 884 885 886 887 889 890 891 892 894 895 897 898 899 901 903 904 905 906\n",
      " 907 908 909 911 912 913 914 915 916 917 918 920 922 923 924 925 927 929\n",
      " 930 931 932 933 935 936 937 938 939 940 941 942 943 944 945 946 948 949\n",
      " 951 952 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969\n",
      " 970 971 972 973 974 976 977 978 979 981 982 983 984 986 987 988 989 990\n",
      " 991 992 993 994 996 997 998 999] TEST :  [  6   9  11  17  27  33  36  39  44  45  57  64  79  82  85  87  92  93\n",
      "  95  98 104 105 106 115 127 133 140 144 145 147 149 151 153 162 170 184\n",
      " 186 189 190 191 196 208 210 215 220 228 229 233 234 237 243 252 253 254\n",
      " 256 259 260 261 264 269 270 274 277 278 282 283 290 298 301 303 306 313\n",
      " 320 332 337 339 341 349 350 357 358 362 368 380 383 389 390 395 403 408\n",
      " 410 413 415 420 421 422 428 430 435 448 449 469 494 495 497 500 501 507\n",
      " 516 517 523 531 533 541 547 548 554 560 566 569 582 583 586 591 594 598\n",
      " 602 603 605 610 611 613 623 625 627 629 630 631 634 657 665 667 668 676\n",
      " 693 695 713 718 721 722 723 729 735 754 770 774 775 776 778 780 782 789\n",
      " 790 792 793 796 799 801 806 814 816 836 837 842 843 845 850 852 853 861\n",
      " 866 870 888 893 896 900 902 910 919 921 926 928 934 947 950 953 975 980\n",
      " 985 995]\n",
      "TRAIN :  [  1   2   3   4   5   6   7   9  10  11  12  13  14  15  16  17  18  20\n",
      "  22  23  24  25  27  28  29  30  32  33  35  36  37  38  39  40  41  42\n",
      "  44  45  48  52  53  54  55  57  58  60  62  64  65  66  67  68  69  70\n",
      "  71  72  73  74  76  77  78  79  80  82  84  85  87  89  90  91  92  93\n",
      "  94  95  98  99 100 101 103 104 105 106 107 108 109 110 111 112 113 114\n",
      " 115 116 117 118 119 120 123 126 127 128 129 130 131 133 134 136 137 139\n",
      " 140 141 142 143 144 145 146 147 149 150 151 152 153 154 155 156 157 158\n",
      " 159 160 161 162 163 164 165 166 167 169 170 171 172 173 174 175 176 177\n",
      " 178 179 180 181 182 183 184 185 186 188 189 190 191 192 193 194 196 197\n",
      " 198 199 200 202 203 204 205 206 208 209 210 212 213 214 215 216 217 220\n",
      " 221 222 223 225 226 227 228 229 230 231 232 233 234 235 236 237 239 240\n",
      " 241 242 243 244 246 247 248 249 250 251 252 253 254 256 257 258 259 260\n",
      " 261 262 264 265 266 267 268 269 270 272 273 274 275 276 277 278 279 280\n",
      " 281 282 283 284 285 286 288 289 290 291 292 293 294 295 297 298 300 301\n",
      " 303 304 305 306 307 308 309 310 311 312 313 314 315 317 318 320 321 322\n",
      " 327 328 329 330 331 332 333 334 335 337 338 339 340 341 342 343 344 345\n",
      " 346 347 349 350 352 353 354 355 356 357 358 362 363 365 368 369 370 371\n",
      " 372 373 374 375 376 377 378 379 380 381 382 383 384 386 387 388 389 390\n",
      " 391 393 394 395 396 397 398 399 400 402 403 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 425 426 427 428 429 430 431 432\n",
      " 435 436 437 438 439 440 441 443 444 445 447 448 449 451 452 453 455 456\n",
      " 457 458 459 460 462 463 464 465 467 468 469 470 471 472 473 475 476 479\n",
      " 480 481 484 486 488 489 490 492 494 495 496 497 499 500 501 502 503 504\n",
      " 505 507 508 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524\n",
      " 525 526 529 530 531 532 533 535 536 537 540 541 542 543 544 546 547 548\n",
      " 549 551 552 553 554 556 557 558 559 560 563 565 566 567 568 569 571 572\n",
      " 573 575 576 577 578 579 580 581 582 583 586 589 591 593 594 595 596 597\n",
      " 598 599 600 601 602 603 605 606 608 610 611 612 613 614 615 616 617 618\n",
      " 620 621 623 624 625 626 627 629 630 631 632 633 634 636 637 638 639 640\n",
      " 641 642 643 644 645 646 650 652 653 654 656 657 658 659 660 662 663 664\n",
      " 665 666 667 668 669 671 672 673 675 676 677 678 681 682 683 684 685 686\n",
      " 687 688 690 691 692 693 694 695 696 698 699 700 701 702 704 705 706 707\n",
      " 710 711 712 713 715 716 717 718 719 721 722 723 724 726 727 728 729 731\n",
      " 732 733 734 735 737 738 739 740 742 743 744 747 748 749 752 753 754 755\n",
      " 756 757 763 765 768 770 771 773 774 775 776 777 778 779 780 781 782 783\n",
      " 786 787 788 789 790 791 792 793 794 795 796 797 798 799 801 802 803 806\n",
      " 809 810 811 812 813 814 815 816 818 819 821 822 824 825 826 827 828 829\n",
      " 830 832 833 834 835 836 837 839 840 842 843 844 845 846 848 850 851 852\n",
      " 853 854 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871\n",
      " 872 874 876 877 878 879 880 881 882 883 884 886 887 888 889 890 891 893\n",
      " 894 895 896 897 898 899 900 901 902 905 906 907 908 909 910 911 912 913\n",
      " 915 916 919 920 921 923 924 925 926 927 928 929 930 931 932 933 934 935\n",
      " 936 937 940 941 944 946 947 948 949 950 951 952 953 954 955 957 959 960\n",
      " 962 963 964 965 967 968 969 973 974 975 976 977 978 979 980 982 984 985\n",
      " 986 989 990 991 995 996 998 999] TEST :  [  0   8  19  21  26  31  34  43  46  47  49  50  51  56  59  61  63  75\n",
      "  81  83  86  88  96  97 102 121 122 124 125 132 135 138 148 168 187 195\n",
      " 201 207 211 218 219 224 238 245 255 263 271 287 296 299 302 316 319 323\n",
      " 324 325 326 336 348 351 359 360 361 364 366 367 385 392 401 404 405 406\n",
      " 424 433 434 442 446 450 454 461 466 474 477 478 482 483 485 487 491 493\n",
      " 498 506 509 527 528 534 538 539 545 550 555 561 562 564 570 574 584 585\n",
      " 587 588 590 592 604 607 609 619 622 628 635 647 648 649 651 655 661 670\n",
      " 674 679 680 689 697 703 708 709 714 720 725 730 736 741 745 746 750 751\n",
      " 758 759 760 761 762 764 766 767 769 772 784 785 800 804 805 807 808 817\n",
      " 820 823 831 838 841 847 849 855 873 875 885 892 903 904 914 917 918 922\n",
      " 938 939 942 943 945 956 958 961 966 970 971 972 981 983 987 988 992 993\n",
      " 994 997]\n"
     ]
    }
   ],
   "source": [
    "for train_index,test_index in kf.split(x_data):\n",
    "    print(\"TRAIN : \",train_index,\"TEST : \",test_index)\n",
    "    x_train,x_test=x_data[train_index],x_data[test_index]\n",
    "    y_train,y_test=y_data[train_index],y_data[test_index]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf08ea5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.78125 0.8     0.71875 0.70625 0.7125 ]\n",
      "74.37499999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression()\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(lr, x_train,y_train, cv=kf)\n",
    "print(scores)\n",
    "print(np.mean(scores)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e805d016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 1], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "y_pred=cross_val_predict(lr,x_test,y_test,cv=kf)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ae308e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracu score :  70.0\n",
      "confusion matrix :\n",
      "[[65 29]\n",
      " [31 75]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "ac = accuracy_score(y_test,y_pred)*100\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print(\"accuracu score : \",ac)\n",
    "print(\"confusion matrix :\")\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
