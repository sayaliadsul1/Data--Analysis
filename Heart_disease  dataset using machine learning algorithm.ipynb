{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e44219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3346f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(r\"C:\\Users\\Acer\\Desktop\\Dataset\\heart_disease dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa22f059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>203</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>294</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>221</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>275</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1025 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0      52    1   0       125   212    0        1      168      0      1.0   \n",
       "1      53    1   0       140   203    1        0      155      1      3.1   \n",
       "2      70    1   0       145   174    0        1      125      1      2.6   \n",
       "3      61    1   0       148   203    0        1      161      0      0.0   \n",
       "4      62    0   0       138   294    1        1      106      0      1.9   \n",
       "...   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n",
       "1020   59    1   1       140   221    0        1      164      1      0.0   \n",
       "1021   60    1   0       125   258    0        0      141      1      2.8   \n",
       "1022   47    1   0       110   275    0        0      118      1      1.0   \n",
       "1023   50    0   0       110   254    0        0      159      0      0.0   \n",
       "1024   54    1   0       120   188    0        1      113      0      1.4   \n",
       "\n",
       "      slope  ca  thal  target  \n",
       "0         2   2     3       0  \n",
       "1         0   0     3       0  \n",
       "2         0   0     3       0  \n",
       "3         2   1     3       0  \n",
       "4         1   3     2       0  \n",
       "...     ...  ..   ...     ...  \n",
       "1020      2   0     2       1  \n",
       "1021      1   1     3       0  \n",
       "1022      1   1     2       0  \n",
       "1023      2   0     2       1  \n",
       "1024      1   1     3       0  \n",
       "\n",
       "[1025 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "313e4b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1025 entries, 0 to 1024\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       1025 non-null   int64  \n",
      " 1   sex       1025 non-null   int64  \n",
      " 2   cp        1025 non-null   int64  \n",
      " 3   trestbps  1025 non-null   int64  \n",
      " 4   chol      1025 non-null   int64  \n",
      " 5   fbs       1025 non-null   int64  \n",
      " 6   restecg   1025 non-null   int64  \n",
      " 7   thalach   1025 non-null   int64  \n",
      " 8   exang     1025 non-null   int64  \n",
      " 9   oldpeak   1025 non-null   float64\n",
      " 10  slope     1025 non-null   int64  \n",
      " 11  ca        1025 non-null   int64  \n",
      " 12  thal      1025 non-null   int64  \n",
      " 13  target    1025 non-null   int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 112.2 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7f624e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "cp          0\n",
       "trestbps    0\n",
       "chol        0\n",
       "fbs         0\n",
       "restecg     0\n",
       "thalach     0\n",
       "exang       0\n",
       "oldpeak     0\n",
       "slope       0\n",
       "ca          0\n",
       "thal        0\n",
       "target      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f62bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data.iloc[:,:-1].values\n",
    "y=data.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7c5da6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "s=SMOTE()\n",
    "x_data,y_data=s.fit_resample(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79c2d69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Original data : Counter({1: 526, 0: 499})\n",
      "The Artificial data : Counter({0: 526, 1: 526})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"The Original data :\",Counter(y))\n",
    "print(\"The Artificial data :\",Counter(y_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bdcc21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=2, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf=KFold(n_splits=5,random_state=2,shuffle=True)\n",
    "kf.get_n_splits(x_data)\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efc4683d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN :  [   0    1    2    3    4    5    6    8    9   10   11   12   13   14\n",
      "   16   17   19   21   22   23   24   25   26   27   28   29   30   31\n",
      "   32   33   34   35   36   38   39   40   41   42   43   44   45   46\n",
      "   48   49   50   51   52   53   54   55   56   57   58   59   61   62\n",
      "   63   64   69   72   73   75   76   77   78   79   80   81   82   83\n",
      "   85   86   87   88   90   92   93   94   95   96   97   98  100  103\n",
      "  104  105  106  107  108  109  110  111  112  113  114  115  116  117\n",
      "  118  119  120  121  122  123  124  125  126  127  128  129  130  131\n",
      "  132  133  134  135  136  138  139  140  142  144  145  147  148  149\n",
      "  150  151  152  153  154  156  157  159  160  161  162  163  165  166\n",
      "  167  168  169  170  171  172  173  174  175  176  177  178  179  181\n",
      "  184  186  187  188  189  190  191  195  196  198  201  202  203  204\n",
      "  205  206  207  208  209  210  211  214  215  216  217  218  219  220\n",
      "  221  222  224  225  227  228  229  231  233  234  236  237  238  239\n",
      "  240  242  243  244  245  246  247  248  249  252  253  254  255  256\n",
      "  257  258  259  260  261  262  263  264  266  268  269  270  271  274\n",
      "  275  276  277  278  279  280  281  282  283  285  289  290  292  294\n",
      "  296  297  298  299  300  301  302  303  304  305  306  307  308  313\n",
      "  314  316  317  318  319  320  321  324  325  326  327  328  329  331\n",
      "  332  333  334  336  337  338  339  340  341  342  344  346  348  349\n",
      "  350  352  353  354  357  358  359  360  361  362  364  366  367  368\n",
      "  369  370  371  372  373  375  377  378  379  380  381  382  383  385\n",
      "  386  387  388  389  390  392  393  394  395  396  397  399  400  401\n",
      "  402  403  404  405  406  407  408  409  410  411  412  413  414  415\n",
      "  416  418  420  421  422  424  425  427  428  429  430  432  433  434\n",
      "  435  438  439  440  441  442  443  444  445  446  447  448  449  450\n",
      "  451  453  454  455  456  459  460  461  463  464  465  466  467  469\n",
      "  470  472  474  475  477  478  479  480  481  482  483  485  486  487\n",
      "  489  491  492  493  494  495  496  497  498  499  500  501  502  503\n",
      "  504  506  507  508  509  510  512  514  516  517  518  519  520  523\n",
      "  525  526  527  528  529  531  532  533  534  535  537  538  539  540\n",
      "  541  542  543  544  545  546  547  548  549  550  551  552  553  554\n",
      "  555  557  559  560  561  562  563  566  567  568  569  570  572  573\n",
      "  574  575  576  577  578  579  580  582  583  584  585  586  587  588\n",
      "  590  591  593  594  596  598  599  600  602  603  604  605  606  607\n",
      "  608  609  610  611  612  613  614  615  616  618  619  620  621  622\n",
      "  623  625  626  627  628  629  630  631  632  633  634  635  636  637\n",
      "  638  640  641  643  646  647  648  649  650  651  652  653  655  656\n",
      "  657  660  661  662  663  664  666  667  668  670  671  672  673  674\n",
      "  675  676  677  678  679  680  681  684  685  686  687  689  690  691\n",
      "  692  693  694  696  697  698  699  700  701  703  705  707  709  710\n",
      "  713  714  716  717  718  719  720  721  722  723  724  725  726  727\n",
      "  729  730  733  734  735  736  737  739  740  741  742  744  745  746\n",
      "  749  750  751  752  753  754  755  757  758  759  760  761  762  764\n",
      "  766  767  768  769  770  772  773  774  775  776  777  778  779  781\n",
      "  782  783  784  785  787  788  789  790  791  793  794  795  796  797\n",
      "  798  799  800  801  803  804  805  806  807  808  809  810  811  813\n",
      "  815  816  817  818  819  820  822  823  824  825  826  827  831  832\n",
      "  833  834  835  837  838  839  840  841  842  843  844  846  847  848\n",
      "  849  850  851  852  854  855  856  857  859  860  861  863  864  866\n",
      "  868  871  872  873  874  875  876  878  879  881  882  884  885  886\n",
      "  887  889  890  891  892  894  895  897  899  900  902  903  904  905\n",
      "  906  907  908  909  911  912  913  914  915  916  917  918  919  921\n",
      "  923  924  925  927  929  930  931  932  933  934  935  937  938  939\n",
      "  940  941  942  943  945  947  948  949  950  953  956  957  959  961\n",
      "  962  963  966  967  968  969  970  971  972  973  976  977  978  979\n",
      "  980  981  982  985  986  987  989  990  991  993  994  995  996  997\n",
      "  998  999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1010 1012 1014\n",
      " 1015 1016 1018 1019 1020 1021 1023 1024 1025 1026 1028 1029 1031 1032\n",
      " 1033 1034 1037 1038 1039 1040 1041 1042 1044 1045 1046 1047 1048 1049\n",
      " 1050] TEST :  [   7   15   18   20   37   47   60   65   66   67   68   70   71   74\n",
      "   84   89   91   99  101  102  137  141  143  146  155  158  164  180\n",
      "  182  183  185  192  193  194  197  199  200  212  213  223  226  230\n",
      "  232  235  241  250  251  265  267  272  273  284  286  287  288  291\n",
      "  293  295  309  310  311  312  315  322  323  330  335  343  345  347\n",
      "  351  355  356  363  365  374  376  384  391  398  417  419  423  426\n",
      "  431  436  437  452  457  458  462  468  471  473  476  484  488  490\n",
      "  505  511  513  515  521  522  524  530  536  556  558  564  565  571\n",
      "  581  589  592  595  597  601  617  624  639  642  644  645  654  658\n",
      "  659  665  669  682  683  688  695  702  704  706  708  711  712  715\n",
      "  728  731  732  738  743  747  748  756  763  765  771  780  786  792\n",
      "  802  812  814  821  828  829  830  836  845  853  858  862  865  867\n",
      "  869  870  877  880  883  888  893  896  898  901  910  920  922  926\n",
      "  928  936  944  946  951  952  954  955  958  960  964  965  974  975\n",
      "  983  984  988  992 1009 1011 1013 1017 1022 1027 1030 1035 1036 1043\n",
      " 1051]\n",
      "TRAIN :  [   0    3    5    6    7    8    9   11   14   15   16   17   18   19\n",
      "   20   21   23   24   25   26   27   28   29   31   32   33   34   35\n",
      "   36   37   38   39   41   43   44   45   46   47   48   49   50   51\n",
      "   52   53   54   56   57   58   59   60   61   63   64   65   66   67\n",
      "   68   69   70   71   73   74   75   76   78   79   81   82   83   84\n",
      "   85   86   87   88   89   91   92   93   94   95   96   97   98   99\n",
      "  100  101  102  103  104  105  106  110  112  113  114  115  118  120\n",
      "  121  122  123  124  125  127  130  131  132  133  135  136  137  138\n",
      "  139  140  141  143  144  145  146  147  148  149  150  151  153  154\n",
      "  155  156  158  159  161  162  164  165  166  167  168  170  171  173\n",
      "  176  178  180  182  183  184  185  186  187  189  190  191  192  193\n",
      "  194  195  196  197  199  200  201  203  206  207  208  210  211  212\n",
      "  213  214  215  217  218  219  220  221  222  223  224  225  226  228\n",
      "  229  230  232  233  234  235  237  238  241  242  243  244  245  246\n",
      "  247  249  250  251  252  253  254  255  256  259  260  261  262  263\n",
      "  264  265  266  267  268  269  270  271  272  273  274  276  277  278\n",
      "  282  283  284  285  286  287  288  289  290  291  292  293  295  296\n",
      "  297  298  299  300  301  302  303  306  307  309  310  311  312  313\n",
      "  314  315  316  319  320  321  322  323  324  325  326  328  329  330\n",
      "  331  332  334  335  336  337  339  341  342  343  345  347  348  349\n",
      "  350  351  352  354  355  356  357  358  359  360  361  362  363  364\n",
      "  365  366  367  368  369  371  372  373  374  376  377  378  380  381\n",
      "  382  383  384  385  386  387  389  390  391  392  393  394  395  398\n",
      "  401  403  404  405  406  408  410  413  415  416  417  419  420  421\n",
      "  422  423  424  425  426  428  429  430  431  433  434  435  436  437\n",
      "  439  442  443  444  445  446  447  448  449  450  451  452  453  454\n",
      "  457  458  461  462  466  467  468  469  471  472  473  474  475  476\n",
      "  477  478  479  482  483  484  485  486  487  488  489  490  491  493\n",
      "  494  495  497  498  499  500  501  503  504  505  506  507  509  511\n",
      "  512  513  514  515  516  517  519  520  521  522  523  524  526  527\n",
      "  528  530  531  533  534  535  536  537  538  539  541  545  547  548\n",
      "  549  550  551  553  554  555  556  557  558  559  560  561  562  563\n",
      "  564  565  566  567  568  569  570  571  574  575  577  581  582  583\n",
      "  584  585  586  587  588  589  590  591  592  593  594  595  597  598\n",
      "  601  602  603  604  605  606  607  609  610  611  612  613  615  616\n",
      "  617  619  621  622  623  624  625  626  627  629  630  631  632  633\n",
      "  634  635  636  637  638  639  642  644  645  647  648  649  650  651\n",
      "  652  653  654  655  656  657  658  659  660  661  662  663  665  666\n",
      "  667  668  669  670  672  673  674  675  676  677  678  679  680  681\n",
      "  682  683  684  685  686  688  689  693  694  695  696  697  698  699\n",
      "  702  703  704  705  706  707  708  709  710  711  712  713  714  715\n",
      "  716  717  718  719  720  721  722  723  724  725  726  728  729  730\n",
      "  731  732  736  738  740  741  743  744  745  746  747  748  750  751\n",
      "  752  753  754  755  756  758  759  760  761  762  763  764  765  766\n",
      "  767  768  769  771  772  773  774  775  777  778  779  780  781  783\n",
      "  784  785  786  788  789  790  791  792  794  795  800  801  802  803\n",
      "  804  805  807  808  809  810  812  813  814  817  818  820  821  822\n",
      "  823  825  827  828  829  830  831  832  833  834  836  837  838  839\n",
      "  840  841  842  844  845  847  848  850  852  853  854  855  856  857\n",
      "  858  859  860  862  863  865  867  868  869  870  871  872  873  874\n",
      "  875  876  877  880  882  883  884  885  886  887  888  889  890  892\n",
      "  893  894  896  897  898  900  901  902  904  905  906  907  908  909\n",
      "  910  911  912  913  914  915  916  917  918  919  920  921  922  923\n",
      "  924  925  926  928  929  930  932  933  934  936  938  939  941  942\n",
      "  943  944  945  946  949  951  952  954  955  957  958  959  960  961\n",
      "  962  964  965  966  967  968  969  972  973  974  975  976  978  979\n",
      "  980  981  983  984  986  987  988  989  992  993  995  997  998  999\n",
      " 1001 1002 1003 1004 1007 1009 1010 1011 1012 1013 1014 1015 1016 1017\n",
      " 1018 1019 1021 1022 1025 1026 1027 1028 1029 1030 1032 1033 1034 1035\n",
      " 1036 1037 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050\n",
      " 1051] TEST :  [   1    2    4   10   12   13   22   30   40   42   55   62   72   77\n",
      "   80   90  107  108  109  111  116  117  119  126  128  129  134  142\n",
      "  152  157  160  163  169  172  174  175  177  179  181  188  198  202\n",
      "  204  205  209  216  227  231  236  239  240  248  257  258  275  279\n",
      "  280  281  294  304  305  308  317  318  327  333  338  340  344  346\n",
      "  353  370  375  379  388  396  397  399  400  402  407  409  411  412\n",
      "  414  418  427  432  438  440  441  455  456  459  460  463  464  465\n",
      "  470  480  481  492  496  502  508  510  518  525  529  532  540  542\n",
      "  543  544  546  552  572  573  576  578  579  580  596  599  600  608\n",
      "  614  618  620  628  640  641  643  646  664  671  687  690  691  692\n",
      "  700  701  727  733  734  735  737  739  742  749  757  770  776  782\n",
      "  787  793  796  797  798  799  806  811  815  816  819  824  826  835\n",
      "  843  846  849  851  861  864  866  878  879  881  891  895  899  903\n",
      "  927  931  935  937  940  947  948  950  953  956  963  970  971  977\n",
      "  982  985  990  991  994  996 1000 1005 1006 1008 1020 1023 1024 1031\n",
      " 1038]\n",
      "TRAIN :  [   0    1    2    4    6    7    8    9   10   11   12   13   15   17\n",
      "   18   19   20   21   22   26   27   30   31   33   34   36   37   39\n",
      "   40   42   43   44   45   46   47   49   51   55   56   57   59   60\n",
      "   61   62   63   64   65   66   67   68   70   71   72   74   75   77\n",
      "   79   80   81   82   83   84   85   86   87   88   89   90   91   92\n",
      "   93   95   96   97   98   99  101  102  104  105  106  107  108  109\n",
      "  111  115  116  117  119  121  122  124  125  126  127  128  129  132\n",
      "  133  134  135  137  138  140  141  142  143  144  145  146  147  148\n",
      "  149  151  152  153  155  157  158  160  162  163  164  169  170  172\n",
      "  174  175  177  179  180  181  182  183  184  185  186  187  188  189\n",
      "  190  191  192  193  194  195  196  197  198  199  200  201  202  204\n",
      "  205  207  208  209  210  211  212  213  215  216  218  219  220  223\n",
      "  224  226  227  228  229  230  231  232  233  234  235  236  237  238\n",
      "  239  240  241  243  245  247  248  250  251  252  253  254  255  256\n",
      "  257  258  259  260  261  264  265  267  269  270  271  272  273  274\n",
      "  275  277  278  279  280  281  282  283  284  286  287  288  290  291\n",
      "  293  294  295  296  298  299  301  303  304  305  306  307  308  309\n",
      "  310  311  312  313  315  316  317  318  319  320  322  323  324  326\n",
      "  327  330  332  333  335  336  337  338  339  340  341  343  344  345\n",
      "  346  347  348  349  350  351  353  355  356  357  358  359  360  361\n",
      "  362  363  364  365  366  367  368  370  373  374  375  376  379  380\n",
      "  383  384  385  388  389  390  391  392  395  396  397  398  399  400\n",
      "  401  402  403  404  405  406  407  408  409  410  411  412  413  414\n",
      "  415  417  418  419  420  421  422  423  424  426  427  428  430  431\n",
      "  432  433  434  435  436  437  438  440  441  442  446  448  449  450\n",
      "  452  454  455  456  457  458  459  460  461  462  463  464  465  466\n",
      "  468  469  470  471  472  473  474  476  477  478  480  481  482  483\n",
      "  484  485  487  488  490  491  492  493  494  495  496  497  498  499\n",
      "  500  501  502  505  506  507  508  509  510  511  513  515  516  517\n",
      "  518  521  522  523  524  525  527  528  529  530  531  532  533  534\n",
      "  536  538  539  540  541  542  543  544  546  547  548  552  554  555\n",
      "  556  558  560  561  562  563  564  565  566  569  570  571  572  573\n",
      "  574  576  578  579  580  581  582  583  585  586  587  588  589  590\n",
      "  591  592  594  595  596  597  598  599  600  601  602  603  604  605\n",
      "  607  608  609  610  611  613  614  616  617  618  619  620  622  623\n",
      "  624  625  626  627  628  630  631  634  635  636  638  639  640  641\n",
      "  642  643  644  645  646  647  648  649  650  651  654  655  657  658\n",
      "  659  661  664  665  668  669  670  671  674  675  676  677  679  680\n",
      "  681  682  683  686  687  688  689  690  691  692  693  695  697  698\n",
      "  699  700  701  702  703  704  706  708  709  711  712  714  715  716\n",
      "  718  719  720  722  723  725  726  727  728  729  730  731  732  733\n",
      "  734  735  736  737  738  739  741  742  743  745  746  747  748  749\n",
      "  750  751  753  754  756  757  758  759  760  761  762  763  764  765\n",
      "  766  767  769  770  771  772  773  774  775  776  777  778  780  782\n",
      "  784  785  786  787  788  789  790  792  793  795  796  797  798  799\n",
      "  800  801  802  804  805  806  807  808  811  812  813  814  815  816\n",
      "  819  820  821  823  824  825  826  827  828  829  830  831  833  834\n",
      "  835  836  838  840  841  842  843  844  845  846  847  849  850  851\n",
      "  853  854  855  856  857  858  859  860  861  862  863  864  865  866\n",
      "  867  868  869  870  871  872  873  875  876  877  878  879  880  881\n",
      "  882  883  884  885  887  888  891  892  893  895  896  897  898  899\n",
      "  901  903  904  906  907  909  910  911  914  915  916  917  918  919\n",
      "  920  921  922  924  925  926  927  928  930  931  932  933  934  935\n",
      "  936  937  938  940  942  943  944  946  947  948  949  950  951  952\n",
      "  953  954  955  956  957  958  959  960  961  962  963  964  965  969\n",
      "  970  971  972  973  974  975  976  977  978  979  980  981  982  983\n",
      "  984  985  986  988  990  991  992  993  994  995  996  997  998  999\n",
      " 1000 1001 1002 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014\n",
      " 1015 1017 1019 1020 1021 1022 1023 1024 1025 1026 1027 1030 1031 1032\n",
      " 1033 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1048 1049\n",
      " 1050 1051] TEST :  [   3    5   14   16   23   24   25   28   29   32   35   38   41   48\n",
      "   50   52   53   54   58   69   73   76   78   94  100  103  110  112\n",
      "  113  114  118  120  123  130  131  136  139  150  154  156  159  161\n",
      "  165  166  167  168  171  173  176  178  203  206  214  217  221  222\n",
      "  225  242  244  246  249  262  263  266  268  276  285  289  292  297\n",
      "  300  302  314  321  325  328  329  331  334  342  352  354  369  371\n",
      "  372  377  378  381  382  386  387  393  394  416  425  429  439  443\n",
      "  444  445  447  451  453  467  475  479  486  489  503  504  512  514\n",
      "  519  520  526  535  537  545  549  550  551  553  557  559  567  568\n",
      "  575  577  584  593  606  612  615  621  629  632  633  637  652  653\n",
      "  656  660  662  663  666  667  672  673  678  684  685  694  696  705\n",
      "  707  710  713  717  721  724  740  744  752  755  768  779  781  783\n",
      "  791  794  803  809  810  817  818  822  832  837  839  848  852  874\n",
      "  886  889  890  894  900  902  905  908  912  913  923  929  939  941\n",
      "  945  966  967  968  987  989 1003 1016 1018 1028 1029 1034 1046 1047]\n",
      "TRAIN :  [   0    1    2    3    4    5    7    8    9   10   12   13   14   15\n",
      "   16   18   19   20   21   22   23   24   25   26   28   29   30   31\n",
      "   32   34   35   36   37   38   40   41   42   43   45   46   47   48\n",
      "   49   50   51   52   53   54   55   56   58   59   60   61   62   63\n",
      "   65   66   67   68   69   70   71   72   73   74   75   76   77   78\n",
      "   80   81   83   84   86   88   89   90   91   94   95   96   97   99\n",
      "  100  101  102  103  107  108  109  110  111  112  113  114  116  117\n",
      "  118  119  120  121  122  123  124  125  126  128  129  130  131  132\n",
      "  134  135  136  137  138  139  141  142  143  146  148  149  150  152\n",
      "  154  155  156  157  158  159  160  161  163  164  165  166  167  168\n",
      "  169  171  172  173  174  175  176  177  178  179  180  181  182  183\n",
      "  184  185  187  188  192  193  194  197  198  199  200  201  202  203\n",
      "  204  205  206  207  209  211  212  213  214  216  217  218  219  221\n",
      "  222  223  224  225  226  227  230  231  232  235  236  239  240  241\n",
      "  242  244  245  246  248  249  250  251  255  257  258  262  263  265\n",
      "  266  267  268  271  272  273  275  276  279  280  281  284  285  286\n",
      "  287  288  289  290  291  292  293  294  295  296  297  298  299  300\n",
      "  302  304  305  308  309  310  311  312  313  314  315  316  317  318\n",
      "  319  321  322  323  324  325  326  327  328  329  330  331  333  334\n",
      "  335  336  338  339  340  342  343  344  345  346  347  348  351  352\n",
      "  353  354  355  356  358  359  360  361  363  364  365  366  367  369\n",
      "  370  371  372  374  375  376  377  378  379  381  382  384  385  386\n",
      "  387  388  391  392  393  394  396  397  398  399  400  401  402  404\n",
      "  405  406  407  409  411  412  414  415  416  417  418  419  420  421\n",
      "  423  424  425  426  427  429  431  432  433  434  436  437  438  439\n",
      "  440  441  442  443  444  445  446  447  450  451  452  453  454  455\n",
      "  456  457  458  459  460  461  462  463  464  465  466  467  468  470\n",
      "  471  473  474  475  476  477  478  479  480  481  482  483  484  485\n",
      "  486  487  488  489  490  491  492  493  496  498  502  503  504  505\n",
      "  506  508  509  510  511  512  513  514  515  518  519  520  521  522\n",
      "  524  525  526  527  528  529  530  532  534  535  536  537  538  539\n",
      "  540  542  543  544  545  546  548  549  550  551  552  553  555  556\n",
      "  557  558  559  560  561  562  564  565  567  568  570  571  572  573\n",
      "  574  575  576  577  578  579  580  581  584  585  587  588  589  590\n",
      "  592  593  595  596  597  599  600  601  604  606  607  608  609  612\n",
      "  614  615  617  618  619  620  621  622  624  628  629  632  633  635\n",
      "  637  639  640  641  642  643  644  645  646  647  648  649  651  652\n",
      "  653  654  655  656  657  658  659  660  661  662  663  664  665  666\n",
      "  667  669  670  671  672  673  674  678  679  680  682  683  684  685\n",
      "  687  688  689  690  691  692  694  695  696  697  700  701  702  703\n",
      "  704  705  706  707  708  709  710  711  712  713  714  715  717  720\n",
      "  721  724  725  727  728  730  731  732  733  734  735  736  737  738\n",
      "  739  740  741  742  743  744  745  746  747  748  749  750  751  752\n",
      "  754  755  756  757  758  759  760  761  762  763  764  765  766  767\n",
      "  768  769  770  771  772  776  779  780  781  782  783  784  785  786\n",
      "  787  791  792  793  794  796  797  798  799  800  802  803  804  805\n",
      "  806  807  808  809  810  811  812  814  815  816  817  818  819  820\n",
      "  821  822  823  824  826  827  828  829  830  831  832  835  836  837\n",
      "  838  839  840  841  843  845  846  848  849  850  851  852  853  855\n",
      "  857  858  861  862  864  865  866  867  868  869  870  873  874  875\n",
      "  877  878  879  880  881  883  885  886  887  888  889  890  891  892\n",
      "  893  894  895  896  897  898  899  900  901  902  903  904  905  907\n",
      "  908  910  911  912  913  914  918  919  920  921  922  923  926  927\n",
      "  928  929  931  933  935  936  937  938  939  940  941  942  944  945\n",
      "  946  947  948  949  950  951  952  953  954  955  956  958  960  963\n",
      "  964  965  966  967  968  970  971  972  974  975  977  978  979  980\n",
      "  982  983  984  985  986  987  988  989  990  991  992  993  994  996\n",
      "  998 1000 1003 1004 1005 1006 1007 1008 1009 1011 1013 1014 1015 1016\n",
      " 1017 1018 1019 1020 1021 1022 1023 1024 1027 1028 1029 1030 1031 1032\n",
      " 1034 1035 1036 1037 1038 1040 1041 1043 1044 1045 1046 1047 1048 1049\n",
      " 1050 1051] TEST :  [   6   11   17   27   33   39   44   57   64   79   82   85   87   92\n",
      "   93   98  104  105  106  115  127  133  140  144  145  147  151  153\n",
      "  162  170  186  189  190  191  195  196  208  210  215  220  228  229\n",
      "  233  234  237  238  243  247  252  253  254  256  259  260  261  264\n",
      "  269  270  274  277  278  282  283  301  303  306  307  320  332  337\n",
      "  341  349  350  357  362  368  373  380  383  389  390  395  403  408\n",
      "  410  413  422  428  430  435  448  449  469  472  494  495  497  499\n",
      "  500  501  507  516  517  523  531  533  541  547  554  563  566  569\n",
      "  582  583  586  591  594  598  602  603  605  610  611  613  616  623\n",
      "  625  626  627  630  631  634  636  638  650  668  675  676  677  681\n",
      "  686  693  698  699  716  718  719  722  723  726  729  753  773  774\n",
      "  775  777  778  788  789  790  795  801  813  825  833  834  842  844\n",
      "  847  854  856  859  860  863  871  872  876  882  884  906  909  915\n",
      "  916  917  924  925  930  932  934  943  957  959  961  962  969  973\n",
      "  976  981  995  997  999 1001 1002 1010 1012 1025 1026 1033 1039 1042]\n",
      "TRAIN :  [   1    2    3    4    5    6    7   10   11   12   13   14   15   16\n",
      "   17   18   20   22   23   24   25   27   28   29   30   32   33   35\n",
      "   37   38   39   40   41   42   44   47   48   50   52   53   54   55\n",
      "   57   58   60   62   64   65   66   67   68   69   70   71   72   73\n",
      "   74   76   77   78   79   80   82   84   85   87   89   90   91   92\n",
      "   93   94   98   99  100  101  102  103  104  105  106  107  108  109\n",
      "  110  111  112  113  114  115  116  117  118  119  120  123  126  127\n",
      "  128  129  130  131  133  134  136  137  139  140  141  142  143  144\n",
      "  145  146  147  150  151  152  153  154  155  156  157  158  159  160\n",
      "  161  162  163  164  165  166  167  168  169  170  171  172  173  174\n",
      "  175  176  177  178  179  180  181  182  183  185  186  188  189  190\n",
      "  191  192  193  194  195  196  197  198  199  200  202  203  204  205\n",
      "  206  208  209  210  212  213  214  215  216  217  220  221  222  223\n",
      "  225  226  227  228  229  230  231  232  233  234  235  236  237  238\n",
      "  239  240  241  242  243  244  246  247  248  249  250  251  252  253\n",
      "  254  256  257  258  259  260  261  262  263  264  265  266  267  268\n",
      "  269  270  272  273  274  275  276  277  278  279  280  281  282  283\n",
      "  284  285  286  287  288  289  291  292  293  294  295  297  300  301\n",
      "  302  303  304  305  306  307  308  309  310  311  312  314  315  317\n",
      "  318  320  321  322  323  325  327  328  329  330  331  332  333  334\n",
      "  335  337  338  340  341  342  343  344  345  346  347  349  350  351\n",
      "  352  353  354  355  356  357  362  363  365  368  369  370  371  372\n",
      "  373  374  375  376  377  378  379  380  381  382  383  384  386  387\n",
      "  388  389  390  391  393  394  395  396  397  398  399  400  402  403\n",
      "  407  408  409  410  411  412  413  414  416  417  418  419  422  423\n",
      "  425  426  427  428  429  430  431  432  435  436  437  438  439  440\n",
      "  441  443  444  445  447  448  449  451  452  453  455  456  457  458\n",
      "  459  460  462  463  464  465  467  468  469  470  471  472  473  475\n",
      "  476  479  480  481  484  486  488  489  490  492  494  495  496  497\n",
      "  499  500  501  502  503  504  505  507  508  510  511  512  513  514\n",
      "  515  516  517  518  519  520  521  522  523  524  525  526  529  530\n",
      "  531  532  533  535  536  537  540  541  542  543  544  545  546  547\n",
      "  549  550  551  552  553  554  556  557  558  559  563  564  565  566\n",
      "  567  568  569  571  572  573  575  576  577  578  579  580  581  582\n",
      "  583  584  586  589  591  592  593  594  595  596  597  598  599  600\n",
      "  601  602  603  605  606  608  610  611  612  613  614  615  616  617\n",
      "  618  620  621  623  624  625  626  627  628  629  630  631  632  633\n",
      "  634  636  637  638  639  640  641  642  643  644  645  646  650  652\n",
      "  653  654  656  658  659  660  662  663  664  665  666  667  668  669\n",
      "  671  672  673  675  676  677  678  681  682  683  684  685  686  687\n",
      "  688  690  691  692  693  694  695  696  698  699  700  701  702  704\n",
      "  705  706  707  708  710  711  712  713  715  716  717  718  719  721\n",
      "  722  723  724  726  727  728  729  731  732  733  734  735  737  738\n",
      "  739  740  742  743  744  747  748  749  752  753  755  756  757  763\n",
      "  765  768  770  771  773  774  775  776  777  778  779  780  781  782\n",
      "  783  786  787  788  789  790  791  792  793  794  795  796  797  798\n",
      "  799  801  802  803  806  809  810  811  812  813  814  815  816  817\n",
      "  818  819  821  822  824  825  826  828  829  830  832  833  834  835\n",
      "  836  837  839  842  843  844  845  846  847  848  849  851  852  853\n",
      "  854  856  858  859  860  861  862  863  864  865  866  867  869  870\n",
      "  871  872  874  876  877  878  879  880  881  882  883  884  886  888\n",
      "  889  890  891  893  894  895  896  898  899  900  901  902  903  905\n",
      "  906  908  909  910  912  913  915  916  917  920  922  923  924  925\n",
      "  926  927  928  929  930  931  932  934  935  936  937  939  940  941\n",
      "  943  944  945  946  947  948  950  951  952  953  954  955  956  957\n",
      "  958  959  960  961  962  963  964  965  966  967  968  969  970  971\n",
      "  973  974  975  976  977  981  982  983  984  985  987  988  989  990\n",
      "  991  992  994  995  996  997  999 1000 1001 1002 1003 1005 1006 1008\n",
      " 1009 1010 1011 1012 1013 1016 1017 1018 1020 1022 1023 1024 1025 1026\n",
      " 1027 1028 1029 1030 1031 1033 1034 1035 1036 1038 1039 1042 1043 1046\n",
      " 1047 1051] TEST :  [   0    8    9   19   21   26   31   34   36   43   45   46   49   51\n",
      "   56   59   61   63   75   81   83   86   88   95   96   97  121  122\n",
      "  124  125  132  135  138  148  149  184  187  201  207  211  218  219\n",
      "  224  245  255  271  290  296  298  299  313  316  319  324  326  336\n",
      "  339  348  358  359  360  361  364  366  367  385  392  401  404  405\n",
      "  406  415  420  421  424  433  434  442  446  450  454  461  466  474\n",
      "  477  478  482  483  485  487  491  493  498  506  509  527  528  534\n",
      "  538  539  548  555  560  561  562  570  574  585  587  588  590  604\n",
      "  607  609  619  622  635  647  648  649  651  655  657  661  670  674\n",
      "  679  680  689  697  703  709  714  720  725  730  736  741  745  746\n",
      "  750  751  754  758  759  760  761  762  764  766  767  769  772  784\n",
      "  785  800  804  805  807  808  820  823  827  831  838  840  841  850\n",
      "  855  857  868  873  875  885  887  892  897  904  907  911  914  918\n",
      "  919  921  933  938  942  949  972  978  979  980  986  993  998 1004\n",
      " 1007 1014 1015 1019 1021 1032 1037 1040 1041 1044 1045 1048 1049 1050]\n"
     ]
    }
   ],
   "source": [
    "for train_index,test_index in kf.split(x_data):\n",
    "    print(\"TRAIN : \",train_index,\"TEST : \",test_index)\n",
    "    x_train,x_test=x_data[train_index],x_data[test_index]\n",
    "    y_train,y_test=y_data[train_index],y_data[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16712166",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12a991bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84023669 0.85798817 0.85714286 0.80952381 0.8452381 ]\n",
      "84.20259227951534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression()\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(lr, x_train,y_train, cv=kf)\n",
    "print(scores)\n",
    "print(np.mean(scores)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66903012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "y1_pred=cross_val_predict(lr,x_test,y_test,cv=kf)\n",
    "y1_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc45b8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score :  84.28571428571429\n",
      "confusion matrix :\n",
      "[[ 75  21]\n",
      " [ 12 102]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "ac1 = accuracy_score(y_test,y1_pred)*100\n",
    "cm1 = confusion_matrix(y_test,y1_pred)\n",
    "print(\"accuracy score : \",ac1)\n",
    "print(\"confusion matrix :\")\n",
    "print(cm1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501ca2f0",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4966171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69230769 0.75739645 0.76785714 0.76190476 0.73809524]\n",
      "74.35122569737955\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "score_1 = cross_val_score(classifier, x_train,y_train, cv=kf)\n",
    "print(score_1)\n",
    "print(np.mean(score_1)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c41b096e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "y2_pred=cross_val_predict(classifier,x_test,y_test,cv=kf)\n",
    "y2_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82205014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score :  65.23809523809524\n",
      "confusion matrix :\n",
      "[[55 41]\n",
      " [32 82]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "ac2 = accuracy_score(y_test,y2_pred)*100\n",
    "cm2= confusion_matrix(y_test,y2_pred)\n",
    "print(\"accuracy score : \",ac2)\n",
    "print(\"confusion matrix :\")\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8eab4e",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257dcc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier1 = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier1.fit(x_train, y_train)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "score_2 = cross_val_score(classifier1, x_train,y_train, cv=kf)\n",
    "print(score_2)\n",
    "print(np.mean(score_2)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd144ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "y3_pred=cross_val_predict(classifier1,x_test,y_test,cv=kf)\n",
    "y3_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5048b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "ac3 = accuracy_score(y_test,y3_pred)*100\n",
    "cm3 = confusion_matrix(y_test,y3_pred)\n",
    "print(\"accuracy score : \",ac3)\n",
    "print(\"confusion matrix :\")\n",
    "print(cm3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
