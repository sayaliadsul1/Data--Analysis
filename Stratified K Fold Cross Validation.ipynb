{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6abb577e",
   "metadata": {},
   "source": [
    "# Stratified K Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da7c992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eca4160",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(r\"C:\\Users\\Acer\\Desktop\\Dataset\\heart_disease dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5d57613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>203</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>294</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>221</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>275</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1025 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0      52    1   0       125   212    0        1      168      0      1.0   \n",
       "1      53    1   0       140   203    1        0      155      1      3.1   \n",
       "2      70    1   0       145   174    0        1      125      1      2.6   \n",
       "3      61    1   0       148   203    0        1      161      0      0.0   \n",
       "4      62    0   0       138   294    1        1      106      0      1.9   \n",
       "...   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n",
       "1020   59    1   1       140   221    0        1      164      1      0.0   \n",
       "1021   60    1   0       125   258    0        0      141      1      2.8   \n",
       "1022   47    1   0       110   275    0        0      118      1      1.0   \n",
       "1023   50    0   0       110   254    0        0      159      0      0.0   \n",
       "1024   54    1   0       120   188    0        1      113      0      1.4   \n",
       "\n",
       "      slope  ca  thal  target  \n",
       "0         2   2     3       0  \n",
       "1         0   0     3       0  \n",
       "2         0   0     3       0  \n",
       "3         2   1     3       0  \n",
       "4         1   3     2       0  \n",
       "...     ...  ..   ...     ...  \n",
       "1020      2   0     2       1  \n",
       "1021      1   1     3       0  \n",
       "1022      1   1     2       0  \n",
       "1023      2   0     2       1  \n",
       "1024      1   1     3       0  \n",
       "\n",
       "[1025 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82fba606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "cp          0\n",
       "trestbps    0\n",
       "chol        0\n",
       "fbs         0\n",
       "restecg     0\n",
       "thalach     0\n",
       "exang       0\n",
       "oldpeak     0\n",
       "slope       0\n",
       "ca          0\n",
       "thal        0\n",
       "target      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82b6795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data.iloc[:,:-1].values\n",
    "y=data.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae5de6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=6, random_state=11, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf=StratifiedKFold(n_splits=6,random_state=11,shuffle=True)\n",
    "skf.get_n_splits(x)\n",
    "print(skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb3c0627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52ed69dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    3    4    5    6    8    9   10   11   12   13   14   15\n",
      "   16   17   18   19   21   22   23   25   26   27   28   29   30   31\n",
      "   33   34   35   36   37   38   39   40   42   43   44   45   46   47\n",
      "   48   49   50   51   52   54   55   57   58   59   61   62   63   65\n",
      "   66   67   68   69   70   71   72   73   74   75   76   77   78   81\n",
      "   82   83   84   85   86   87   88   89   90   93   95   96   97   98\n",
      "   99  100  101  102  103  104  105  106  107  108  109  110  111  112\n",
      "  113  114  115  116  117  118  119  120  121  122  125  126  127  128\n",
      "  129  130  131  132  133  134  135  136  137  138  139  140  142  143\n",
      "  145  146  147  148  149  150  153  154  155  156  157  158  160  161\n",
      "  164  165  166  167  168  169  170  171  172  173  174  175  176  177\n",
      "  178  179  180  181  182  184  186  187  188  189  190  191  192  193\n",
      "  194  195  197  198  200  201  202  203  204  206  207  210  211  212\n",
      "  213  214  215  216  217  218  219  220  221  222  224  225  226  227\n",
      "  229  231  233  234  235  236  237  238  239  240  241  242  243  244\n",
      "  245  246  247  248  249  250  251  252  253  254  256  257  258  261\n",
      "  262  263  264  265  266  267  268  269  270  271  273  274  275  276\n",
      "  277  278  279  280  281  282  283  284  285  287  288  289  290  291\n",
      "  292  293  296  298  300  301  302  303  304  305  306  307  308  309\n",
      "  310  311  312  314  315  316  318  319  320  321  322  324  325  326\n",
      "  327  329  330  331  332  333  334  335  338  339  340  342  343  344\n",
      "  346  348  349  351  352  353  354  355  356  358  359  360  361  363\n",
      "  364  365  366  367  368  369  370  371  372  374  375  376  378  381\n",
      "  382  383  384  385  386  387  388  389  390  391  393  394  395  396\n",
      "  397  398  399  400  401  402  403  404  406  407  408  409  410  411\n",
      "  413  416  417  418  419  420  421  422  423  425  426  427  428  429\n",
      "  430  431  432  433  435  436  438  439  440  441  442  443  446  447\n",
      "  448  450  451  452  453  454  455  456  458  459  460  461  462  464\n",
      "  465  468  470  471  472  473  474  475  476  478  479  480  481  482\n",
      "  483  484  485  486  487  488  489  490  491  492  493  494  495  496\n",
      "  497  498  499  500  501  502  504  507  508  509  511  512  513  514\n",
      "  515  516  518  519  520  521  522  523  524  525  526  527  528  529\n",
      "  530  531  532  533  534  535  536  537  539  541  543  544  545  546\n",
      "  547  549  550  552  553  554  555  557  558  560  561  562  563  564\n",
      "  565  566  567  568  569  570  572  573  575  576  578  579  580  583\n",
      "  584  585  586  587  588  590  592  594  595  596  597  598  599  600\n",
      "  601  602  604  605  606  607  608  610  611  612  613  614  615  616\n",
      "  618  619  620  622  623  624  625  626  627  629  631  632  633  634\n",
      "  635  636  637  638  640  642  644  646  647  648  649  650  651  652\n",
      "  653  656  657  658  659  660  661  663  664  666  667  668  669  670\n",
      "  671  672  673  674  675  676  677  678  679  680  681  683  684  685\n",
      "  687  688  690  691  694  695  696  697  698  699  702  703  704  706\n",
      "  707  708  709  710  711  712  713  715  716  717  718  719  721  723\n",
      "  724  725  726  727  728  729  730  732  733  734  735  736  737  738\n",
      "  739  740  741  742  743  744  746  747  748  749  750  751  752  753\n",
      "  754  756  757  758  761  762  763  764  765  766  767  769  770  771\n",
      "  772  773  774  775  776  777  778  779  780  781  782  783  784  786\n",
      "  787  788  790  791  792  793  794  795  796  797  798  800  801  802\n",
      "  803  804  805  806  807  808  809  810  811  812  813  814  817  819\n",
      "  820  821  822  823  824  825  826  827  829  830  831  832  834  835\n",
      "  837  838  839  840  841  842  843  846  848  849  850  851  852  853\n",
      "  855  856  857  858  859  860  861  862  866  868  869  870  871  872\n",
      "  875  876  877  878  879  880  881  882  883  884  885  886  889  890\n",
      "  892  894  895  896  897  899  900  902  903  904  905  906  907  908\n",
      "  909  910  911  912  913  914  915  916  918  919  921  922  923  924\n",
      "  925  926  927  928  929  931  932  934  935  936  937  938  939  940\n",
      "  942  943  944  946  947  948  949  950  951  953  954  956  958  959\n",
      "  960  961  962  963  964  965  966  967  968  970  971  972  973  974\n",
      "  976  977  978  980  981  982  983  984  986  987  990  991  993  994\n",
      "  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007 1008\n",
      " 1009 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1022 1023 1024] TEST: [   2    7   20   24   32   41   53   56   60   64   79   80   91   92\n",
      "   94  123  124  141  144  151  152  159  162  163  183  185  196  199\n",
      "  205  208  209  223  228  230  232  255  259  260  272  286  294  295\n",
      "  297  299  313  317  323  328  336  337  341  345  347  350  357  362\n",
      "  373  377  379  380  392  405  412  414  415  424  434  437  444  445\n",
      "  449  457  463  466  467  469  477  503  505  506  510  517  538  540\n",
      "  542  548  551  556  559  571  574  577  581  582  589  591  593  603\n",
      "  609  617  621  628  630  639  641  643  645  654  655  662  665  682\n",
      "  686  689  692  693  700  701  705  714  720  722  731  745  755  759\n",
      "  760  768  785  789  799  815  816  818  828  833  836  844  845  847\n",
      "  854  863  864  865  867  873  874  887  888  891  893  898  901  917\n",
      "  920  930  933  941  945  952  955  957  969  975  979  985  988  989\n",
      "  992 1010 1021]\n",
      "[0.88111888 0.87412587 0.82394366 0.81690141 0.85915493 0.84507042]\n",
      "85.00525296299945\n",
      "TRAIN: [   0    1    2    4    5    6    7    8    9   10   11   12   13   14\n",
      "   15   16   17   18   19   20   21   22   23   24   25   26   27   28\n",
      "   29   30   31   32   34   35   36   37   38   39   40   41   42   43\n",
      "   44   46   47   48   49   50   51   52   53   54   55   56   58   59\n",
      "   60   61   62   63   64   66   67   69   70   71   72   73   74   75\n",
      "   76   77   78   79   80   81   82   84   85   86   88   90   91   92\n",
      "   93   94   95   97   99  100  101  103  104  105  106  107  108  109\n",
      "  110  111  112  113  114  115  116  117  118  119  120  122  123  124\n",
      "  125  126  127  129  131  132  133  134  135  136  137  138  139  140\n",
      "  141  142  143  144  145  146  147  148  151  152  153  154  155  156\n",
      "  157  158  159  160  161  162  163  166  167  168  169  170  171  173\n",
      "  174  175  178  180  181  182  183  184  185  186  187  188  190  191\n",
      "  192  193  194  195  196  197  198  199  200  201  202  203  204  205\n",
      "  206  207  208  209  210  211  212  213  214  215  216  218  223  224\n",
      "  226  228  229  230  231  232  233  234  235  236  237  238  239  241\n",
      "  242  243  246  247  248  249  250  251  252  254  255  257  259  260\n",
      "  261  262  263  264  265  266  267  268  269  270  271  272  273  277\n",
      "  278  279  281  282  283  284  285  286  287  289  292  293  294  295\n",
      "  296  297  299  300  303  304  306  307  308  309  310  311  312  313\n",
      "  314  315  316  317  318  319  320  321  322  323  324  325  327  328\n",
      "  331  332  334  335  336  337  339  340  341  342  344  345  346  347\n",
      "  348  350  352  353  354  355  356  357  358  360  362  365  366  367\n",
      "  369  370  371  372  373  374  376  377  378  379  380  383  385  386\n",
      "  387  388  389  390  391  392  393  395  396  398  400  401  404  405\n",
      "  406  408  409  410  411  412  413  414  415  417  418  419  420  421\n",
      "  423  424  425  427  428  429  430  431  432  433  434  435  436  437\n",
      "  438  439  440  441  442  443  444  445  446  447  448  449  450  451\n",
      "  453  455  457  458  459  460  461  463  464  465  466  467  468  469\n",
      "  470  471  473  474  475  477  478  479  480  481  482  483  484  485\n",
      "  486  487  488  490  491  494  496  497  498  499  500  501  502  503\n",
      "  504  505  506  507  508  509  510  511  512  513  515  516  517  519\n",
      "  521  522  523  524  525  526  527  529  530  531  534  535  536  538\n",
      "  539  540  541  542  544  545  546  547  548  549  550  551  553  554\n",
      "  555  556  557  558  559  560  561  562  564  565  566  568  569  570\n",
      "  571  572  574  575  576  577  578  579  580  581  582  583  584  585\n",
      "  586  587  588  589  590  591  593  594  595  596  597  598  599  600\n",
      "  601  602  603  604  605  606  607  609  610  611  612  614  615  617\n",
      "  618  619  620  621  622  623  625  626  628  629  630  632  633  634\n",
      "  635  636  637  638  639  640  641  642  643  644  645  646  647  648\n",
      "  649  650  652  654  655  656  657  658  659  660  662  663  665  666\n",
      "  667  668  669  670  671  672  674  675  676  677  679  680  681  682\n",
      "  683  684  685  686  687  688  689  692  693  694  695  696  697  698\n",
      "  700  701  702  704  705  707  708  709  712  713  714  715  716  717\n",
      "  719  720  721  722  723  724  725  726  727  728  730  731  732  733\n",
      "  734  735  737  738  739  740  741  742  743  745  746  747  748  749\n",
      "  750  751  752  753  754  755  756  757  759  760  762  763  764  766\n",
      "  767  768  769  770  772  774  775  776  777  778  781  782  783  784\n",
      "  785  786  787  788  789  791  792  794  796  797  798  799  801  802\n",
      "  803  805  806  808  810  811  815  816  817  818  819  821  822  823\n",
      "  824  825  828  829  830  831  832  833  834  835  836  837  838  839\n",
      "  840  841  843  844  845  846  847  849  850  852  853  854  855  856\n",
      "  858  859  861  862  863  864  865  866  867  868  869  870  871  872\n",
      "  873  874  875  877  878  880  881  882  883  886  887  888  889  890\n",
      "  891  893  894  895  896  897  898  899  900  901  902  903  904  906\n",
      "  907  908  909  910  911  912  913  914  915  917  918  919  920  921\n",
      "  925  927  928  929  930  932  933  934  935  936  937  938  939  940\n",
      "  941  942  943  944  945  946  947  948  949  950  951  952  954  955\n",
      "  956  957  958  961  962  964  965  966  967  968  969  970  971  972\n",
      "  973  974  975  976  978  979  981  982  983  984  985  986  988  989\n",
      "  990  991  992  994  995  996  997  998  999 1001 1003 1004 1005 1006\n",
      " 1008 1009 1010 1013 1014 1015 1016 1017 1019 1020 1021 1022 1023 1024] TEST: [   3   33   45   57   65   68   83   87   89   96   98  102  121  128\n",
      "  130  149  150  164  165  172  176  177  179  189  217  219  220  221\n",
      "  222  225  227  240  244  245  253  256  258  274  275  276  280  288\n",
      "  290  291  298  301  302  305  326  329  330  333  338  343  349  351\n",
      "  359  361  363  364  368  375  381  382  384  394  397  399  402  403\n",
      "  407  416  422  426  452  454  456  462  472  476  489  492  493  495\n",
      "  514  518  520  528  532  533  537  543  552  563  567  573  592  608\n",
      "  613  616  624  627  631  651  653  661  664  673  678  690  691  699\n",
      "  703  706  710  711  718  729  736  744  758  761  765  771  773  779\n",
      "  780  790  793  795  800  804  807  809  812  813  814  820  826  827\n",
      "  842  848  851  857  860  876  879  884  885  892  905  916  922  923\n",
      "  924  926  931  953  959  960  963  977  980  987  993 1000 1002 1007\n",
      " 1011 1012 1018]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9020979  0.82517483 0.85915493 0.85915493 0.87323944 0.78873239]\n",
      "85.12590695689286\n",
      "TRAIN: [   0    2    3    4    6    7    9   10   11   12   13   14   15   16\n",
      "   17   18   19   20   21   22   24   25   26   27   29   30   31   32\n",
      "   33   34   37   38   39   41   42   43   44   45   46   47   48   49\n",
      "   50   51   52   53   54   55   56   57   58   59   60   62   63   64\n",
      "   65   66   67   68   69   70   72   73   75   76   77   78   79   80\n",
      "   82   83   84   86   87   88   89   90   91   92   94   95   96   97\n",
      "   98   99  101  102  103  104  105  106  107  110  111  112  113  116\n",
      "  117  119  120  121  123  124  125  126  127  128  130  131  132  133\n",
      "  135  136  137  139  140  141  142  143  144  145  146  147  149  150\n",
      "  151  152  153  154  155  157  159  160  162  163  164  165  167  168\n",
      "  169  170  171  172  175  176  177  178  179  180  182  183  184  185\n",
      "  186  188  189  190  192  193  194  196  197  199  200  201  202  203\n",
      "  204  205  206  207  208  209  211  214  215  216  217  218  219  220\n",
      "  221  222  223  225  226  227  228  229  230  231  232  234  235  236\n",
      "  237  238  239  240  241  242  244  245  249  250  251  252  253  254\n",
      "  255  256  257  258  259  260  261  263  266  269  272  273  274  275\n",
      "  276  277  279  280  281  282  283  285  286  288  290  291  292  293\n",
      "  294  295  296  297  298  299  300  301  302  303  304  305  307  308\n",
      "  309  310  311  312  313  314  315  316  317  318  319  320  321  322\n",
      "  323  324  325  326  327  328  329  330  332  333  335  336  337  338\n",
      "  340  341  342  343  344  345  347  348  349  350  351  352  355  357\n",
      "  358  359  360  361  362  363  364  365  366  368  369  370  372  373\n",
      "  374  375  376  377  378  379  380  381  382  383  384  385  386  387\n",
      "  390  391  392  394  395  396  397  399  400  401  402  403  404  405\n",
      "  406  407  408  411  412  413  414  415  416  417  418  419  420  422\n",
      "  424  425  426  427  430  431  433  434  435  437  439  440  442  443\n",
      "  444  445  446  447  449  450  452  453  454  455  456  457  460  461\n",
      "  462  463  465  466  467  468  469  470  471  472  473  474  475  476\n",
      "  477  478  479  482  483  485  486  488  489  490  491  492  493  494\n",
      "  495  496  497  498  499  500  501  502  503  504  505  506  507  508\n",
      "  509  510  511  512  513  514  515  516  517  518  519  520  521  523\n",
      "  524  525  526  527  528  529  530  532  533  534  535  537  538  539\n",
      "  540  541  542  543  544  546  548  549  551  552  553  554  555  556\n",
      "  557  558  559  560  562  563  564  565  567  568  569  570  571  572\n",
      "  573  574  576  577  578  580  581  582  583  584  585  586  587  588\n",
      "  589  590  591  592  593  594  595  596  597  598  599  600  601  602\n",
      "  603  604  605  606  607  608  609  613  614  616  617  618  619  620\n",
      "  621  623  624  625  627  628  630  631  633  634  635  636  637  638\n",
      "  639  640  641  642  643  644  645  646  647  648  650  651  653  654\n",
      "  655  656  657  660  661  662  664  665  666  667  668  669  671  672\n",
      "  673  675  677  678  679  680  681  682  683  684  685  686  687  688\n",
      "  689  690  691  692  693  694  695  696  697  698  699  700  701  702\n",
      "  703  705  706  707  708  709  710  711  713  714  715  716  717  718\n",
      "  719  720  721  722  723  724  728  729  730  731  732  733  734  735\n",
      "  736  737  738  739  740  742  744  745  746  747  748  750  751  752\n",
      "  753  754  755  757  758  759  760  761  763  764  765  766  767  768\n",
      "  769  770  771  772  773  776  777  778  779  780  783  784  785  786\n",
      "  788  789  790  791  792  793  794  795  796  797  798  799  800  802\n",
      "  803  804  806  807  808  809  810  812  813  814  815  816  818  819\n",
      "  820  822  823  824  825  826  827  828  829  831  832  833  834  835\n",
      "  836  837  838  839  840  842  844  845  846  847  848  849  850  851\n",
      "  852  853  854  856  857  859  860  861  862  863  864  865  866  867\n",
      "  868  869  870  871  873  874  875  876  878  879  880  881  882  883\n",
      "  884  885  886  887  888  890  891  892  893  896  897  898  899  901\n",
      "  902  904  905  906  910  912  914  915  916  917  919  920  921  922\n",
      "  923  924  925  926  927  928  929  930  931  932  933  934  935  936\n",
      "  937  938  939  941  945  946  947  948  949  951  952  953  954  955\n",
      "  957  958  959  960  961  962  963  964  965  966  968  969  970  971\n",
      "  972  975  976  977  979  980  981  982  983  985  986  987  988  989\n",
      "  990  991  992  993  995  996  997  998 1000 1001 1002 1003 1005 1007\n",
      " 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1024] TEST: [   1    5    8   23   28   35   36   40   61   71   74   81   85   93\n",
      "  100  108  109  114  115  118  122  129  134  138  148  156  158  161\n",
      "  166  173  174  181  187  191  195  198  210  212  213  224  233  243\n",
      "  246  247  248  262  264  265  267  268  270  271  278  284  287  289\n",
      "  306  331  334  339  346  353  354  356  367  371  388  389  393  398\n",
      "  409  410  421  423  428  429  432  436  438  441  448  451  458  459\n",
      "  464  480  481  484  487  522  531  536  545  547  550  561  566  575\n",
      "  579  610  611  612  615  622  626  629  632  649  652  658  659  663\n",
      "  670  674  676  704  712  725  726  727  741  743  749  756  762  774\n",
      "  775  781  782  787  801  805  811  817  821  830  841  843  855  858\n",
      "  872  877  889  894  895  900  903  907  908  909  911  913  918  940\n",
      "  942  943  944  950  956  967  973  974  978  984  994  999 1004 1006\n",
      " 1008 1022 1023]\n",
      "[0.83916084 0.87412587 0.83098592 0.81690141 0.85211268 0.90140845]\n",
      "85.24491939984897\n",
      "TRAIN: [   1    2    3    4    5    6    7    8   10   13   14   16   17   18\n",
      "   20   21   22   23   24   25   26   28   29   30   31   32   33   35\n",
      "   36   37   38   39   40   41   43   45   47   48   49   51   53   54\n",
      "   55   56   57   60   61   64   65   66   68   70   71   72   73   74\n",
      "   75   78   79   80   81   82   83   84   85   86   87   88   89   90\n",
      "   91   92   93   94   95   96   97   98  100  101  102  103  104  105\n",
      "  107  108  109  113  114  115  116  117  118  119  120  121  122  123\n",
      "  124  126  128  129  130  131  132  133  134  135  136  137  138  139\n",
      "  140  141  142  143  144  145  146  148  149  150  151  152  153  154\n",
      "  156  158  159  161  162  163  164  165  166  167  168  169  170  171\n",
      "  172  173  174  176  177  179  181  183  185  187  188  189  190  191\n",
      "  192  193  195  196  198  199  202  203  204  205  207  208  209  210\n",
      "  211  212  213  215  216  217  218  219  220  221  222  223  224  225\n",
      "  226  227  228  230  231  232  233  234  235  236  238  240  241  242\n",
      "  243  244  245  246  247  248  250  251  252  253  255  256  257  258\n",
      "  259  260  261  262  263  264  265  267  268  269  270  271  272  273\n",
      "  274  275  276  278  279  280  282  284  285  286  287  288  289  290\n",
      "  291  292  293  294  295  296  297  298  299  301  302  304  305  306\n",
      "  307  308  309  312  313  314  315  317  318  319  321  323  325  326\n",
      "  327  328  329  330  331  332  333  334  335  336  337  338  339  340\n",
      "  341  343  344  345  346  347  349  350  351  353  354  356  357  358\n",
      "  359  360  361  362  363  364  366  367  368  369  370  371  372  373\n",
      "  374  375  376  377  378  379  380  381  382  383  384  388  389  390\n",
      "  391  392  393  394  395  396  397  398  399  401  402  403  405  407\n",
      "  408  409  410  412  414  415  416  417  419  420  421  422  423  424\n",
      "  426  427  428  429  430  432  433  434  435  436  437  438  441  442\n",
      "  444  445  447  448  449  450  451  452  454  455  456  457  458  459\n",
      "  460  461  462  463  464  465  466  467  468  469  470  472  474  475\n",
      "  476  477  479  480  481  482  483  484  485  486  487  488  489  490\n",
      "  491  492  493  494  495  496  497  499  501  503  505  506  509  510\n",
      "  511  512  514  516  517  518  519  520  521  522  523  524  526  528\n",
      "  530  531  532  533  535  536  537  538  539  540  541  542  543  544\n",
      "  545  546  547  548  550  551  552  553  556  557  558  559  561  563\n",
      "  564  565  566  567  568  569  571  572  573  574  575  576  577  578\n",
      "  579  580  581  582  584  585  586  588  589  590  591  592  593  597\n",
      "  598  599  600  601  603  604  606  608  609  610  611  612  613  614\n",
      "  615  616  617  618  619  620  621  622  623  624  625  626  627  628\n",
      "  629  630  631  632  634  635  636  637  638  639  640  641  642  643\n",
      "  644  645  648  649  650  651  652  653  654  655  657  658  659  660\n",
      "  661  662  663  664  665  666  668  669  670  671  673  674  676  677\n",
      "  678  679  680  681  682  683  684  685  686  687  688  689  690  691\n",
      "  692  693  694  695  697  698  699  700  701  703  704  705  706  707\n",
      "  708  709  710  711  712  713  714  715  717  718  720  721  722  723\n",
      "  724  725  726  727  728  729  730  731  733  734  735  736  737  738\n",
      "  739  740  741  742  743  744  745  746  747  748  749  752  755  756\n",
      "  757  758  759  760  761  762  763  764  765  766  767  768  769  771\n",
      "  772  773  774  775  776  779  780  781  782  783  784  785  787  788\n",
      "  789  790  791  792  793  794  795  797  798  799  800  801  802  803\n",
      "  804  805  807  808  809  811  812  813  814  815  816  817  818  819\n",
      "  820  821  824  825  826  827  828  829  830  831  832  833  834  835\n",
      "  836  837  838  839  841  842  843  844  845  846  847  848  849  850\n",
      "  851  852  854  855  856  857  858  859  860  862  863  864  865  867\n",
      "  868  869  870  872  873  874  876  877  878  879  881  883  884  885\n",
      "  887  888  889  890  891  892  893  894  895  897  898  900  901  903\n",
      "  904  905  907  908  909  911  912  913  914  916  917  918  920  921\n",
      "  922  923  924  925  926  927  928  929  930  931  932  933  935  937\n",
      "  938  940  941  942  943  944  945  947  948  950  952  953  954  955\n",
      "  956  957  959  960  963  964  965  966  967  968  969  970  971  973\n",
      "  974  975  976  977  978  979  980  982  983  984  985  986  987  988\n",
      "  989  990  991  992  993  994  997  999 1000 1002 1003 1004 1006 1007\n",
      " 1008 1009 1010 1011 1012 1014 1016 1017 1018 1019 1021 1022 1023 1024] TEST: [   0    9   11   12   15   19   27   34   42   44   46   50   52   58\n",
      "   59   62   63   67   69   76   77   99  106  110  111  112  125  127\n",
      "  147  155  157  160  175  178  180  182  184  186  194  197  200  201\n",
      "  206  214  229  237  239  249  254  266  277  281  283  300  303  310\n",
      "  311  316  320  322  324  342  348  352  355  365  385  386  387  400\n",
      "  404  406  411  413  418  425  431  439  440  443  446  453  471  473\n",
      "  478  498  500  502  504  507  508  513  515  525  527  529  534  549\n",
      "  554  555  560  562  570  583  587  594  595  596  602  605  607  633\n",
      "  646  647  656  667  672  675  696  702  716  719  732  750  751  753\n",
      "  754  770  777  778  786  796  806  810  822  823  840  853  861  866\n",
      "  871  875  880  882  886  896  899  902  906  910  915  919  934  936\n",
      "  939  946  949  951  958  961  962  972  981  995  996  998 1001 1005\n",
      " 1013 1015 1020]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.85314685 0.87412587 0.78873239 0.83098592 0.87323944 0.80985915]\n",
      "83.8348271446863\n",
      "TRAIN: [   0    1    2    3    5    7    8    9   10   11   12   13   14   15\n",
      "   17   19   20   22   23   24   25   26   27   28   31   32   33   34\n",
      "   35   36   37   40   41   42   43   44   45   46   47   50   51   52\n",
      "   53   56   57   58   59   60   61   62   63   64   65   67   68   69\n",
      "   70   71   74   76   77   79   80   81   82   83   84   85   86   87\n",
      "   89   90   91   92   93   94   96   98   99  100  101  102  103  104\n",
      "  106  108  109  110  111  112  114  115  118  119  121  122  123  124\n",
      "  125  126  127  128  129  130  132  133  134  136  137  138  141  142\n",
      "  144  147  148  149  150  151  152  153  154  155  156  157  158  159\n",
      "  160  161  162  163  164  165  166  168  170  172  173  174  175  176\n",
      "  177  178  179  180  181  182  183  184  185  186  187  188  189  191\n",
      "  194  195  196  197  198  199  200  201  203  204  205  206  207  208\n",
      "  209  210  211  212  213  214  217  218  219  220  221  222  223  224\n",
      "  225  226  227  228  229  230  232  233  235  236  237  238  239  240\n",
      "  241  243  244  245  246  247  248  249  251  253  254  255  256  258\n",
      "  259  260  261  262  263  264  265  266  267  268  270  271  272  273\n",
      "  274  275  276  277  278  280  281  282  283  284  285  286  287  288\n",
      "  289  290  291  293  294  295  296  297  298  299  300  301  302  303\n",
      "  305  306  308  310  311  312  313  315  316  317  319  320  322  323\n",
      "  324  325  326  327  328  329  330  331  332  333  334  336  337  338\n",
      "  339  341  342  343  345  346  347  348  349  350  351  352  353  354\n",
      "  355  356  357  358  359  361  362  363  364  365  367  368  370  371\n",
      "  372  373  375  376  377  378  379  380  381  382  383  384  385  386\n",
      "  387  388  389  390  392  393  394  397  398  399  400  402  403  404\n",
      "  405  406  407  409  410  411  412  413  414  415  416  417  418  419\n",
      "  420  421  422  423  424  425  426  428  429  431  432  434  436  437\n",
      "  438  439  440  441  442  443  444  445  446  448  449  450  451  452\n",
      "  453  454  455  456  457  458  459  462  463  464  465  466  467  468\n",
      "  469  470  471  472  473  474  475  476  477  478  480  481  483  484\n",
      "  485  486  487  488  489  490  491  492  493  495  498  500  501  502\n",
      "  503  504  505  506  507  508  510  511  512  513  514  515  516  517\n",
      "  518  519  520  521  522  523  524  525  527  528  529  531  532  533\n",
      "  534  535  536  537  538  540  542  543  544  545  546  547  548  549\n",
      "  550  551  552  554  555  556  557  558  559  560  561  562  563  566\n",
      "  567  568  570  571  573  574  575  577  578  579  580  581  582  583\n",
      "  584  586  587  589  591  592  593  594  595  596  598  599  600  601\n",
      "  602  603  604  605  607  608  609  610  611  612  613  615  616  617\n",
      "  618  619  620  621  622  623  624  625  626  627  628  629  630  631\n",
      "  632  633  634  638  639  641  643  645  646  647  649  651  652  653\n",
      "  654  655  656  657  658  659  661  662  663  664  665  666  667  668\n",
      "  670  672  673  674  675  676  678  680  681  682  684  686  688  689\n",
      "  690  691  692  693  696  697  699  700  701  702  703  704  705  706\n",
      "  707  709  710  711  712  713  714  715  716  717  718  719  720  721\n",
      "  722  725  726  727  729  731  732  733  736  737  738  741  742  743\n",
      "  744  745  746  749  750  751  752  753  754  755  756  758  759  760\n",
      "  761  762  763  764  765  766  767  768  769  770  771  773  774  775\n",
      "  776  777  778  779  780  781  782  784  785  786  787  789  790  791\n",
      "  793  795  796  797  799  800  801  803  804  805  806  807  809  810\n",
      "  811  812  813  814  815  816  817  818  820  821  822  823  826  827\n",
      "  828  829  830  831  832  833  834  835  836  837  838  839  840  841\n",
      "  842  843  844  845  847  848  851  852  853  854  855  856  857  858\n",
      "  859  860  861  862  863  864  865  866  867  871  872  873  874  875\n",
      "  876  877  879  880  881  882  884  885  886  887  888  889  891  892\n",
      "  893  894  895  896  898  899  900  901  902  903  904  905  906  907\n",
      "  908  909  910  911  913  914  915  916  917  918  919  920  921  922\n",
      "  923  924  926  927  928  930  931  933  934  936  937  939  940  941\n",
      "  942  943  944  945  946  947  949  950  951  952  953  955  956  957\n",
      "  958  959  960  961  962  963  964  967  968  969  972  973  974  975\n",
      "  976  977  978  979  980  981  983  984  985  987  988  989  990  992\n",
      "  993  994  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006\n",
      " 1007 1008 1010 1011 1012 1013 1015 1016 1018 1019 1020 1021 1022 1023] TEST: [   4    6   16   18   21   29   30   38   39   48   49   54   55   66\n",
      "   72   73   75   78   88   95   97  105  107  113  116  117  120  131\n",
      "  135  139  140  143  145  146  167  169  171  190  192  193  202  215\n",
      "  216  231  234  242  250  252  257  269  279  292  304  307  309  314\n",
      "  318  321  335  340  344  360  366  369  374  391  395  396  401  408\n",
      "  427  430  433  435  447  460  461  479  482  494  496  497  499  509\n",
      "  526  530  539  541  553  564  565  569  572  576  585  588  590  597\n",
      "  606  614  635  636  637  640  642  644  648  650  660  669  671  677\n",
      "  679  683  685  687  694  695  698  708  723  724  728  730  734  735\n",
      "  739  740  747  748  757  772  783  788  792  794  798  802  808  819\n",
      "  824  825  846  849  850  868  869  870  878  883  890  897  912  925\n",
      "  929  932  935  938  948  954  965  966  970  971  982  986  991 1009\n",
      " 1014 1017 1024]\n",
      "[0.84615385 0.83216783 0.83802817 0.79577465 0.82394366 0.84507042]\n",
      "83.01897632883549\n",
      "TRAIN: [   0    1    2    3    4    5    6    7    8    9   11   12   15   16\n",
      "   18   19   20   21   23   24   27   28   29   30   32   33   34   35\n",
      "   36   38   39   40   41   42   44   45   46   48   49   50   52   53\n",
      "   54   55   56   57   58   59   60   61   62   63   64   65   66   67\n",
      "   68   69   71   72   73   74   75   76   77   78   79   80   81   83\n",
      "   85   87   88   89   91   92   93   94   95   96   97   98   99  100\n",
      "  102  105  106  107  108  109  110  111  112  113  114  115  116  117\n",
      "  118  120  121  122  123  124  125  127  128  129  130  131  134  135\n",
      "  138  139  140  141  143  144  145  146  147  148  149  150  151  152\n",
      "  155  156  157  158  159  160  161  162  163  164  165  166  167  169\n",
      "  171  172  173  174  175  176  177  178  179  180  181  182  183  184\n",
      "  185  186  187  189  190  191  192  193  194  195  196  197  198  199\n",
      "  200  201  202  205  206  208  209  210  212  213  214  215  216  217\n",
      "  219  220  221  222  223  224  225  227  228  229  230  231  232  233\n",
      "  234  237  239  240  242  243  244  245  246  247  248  249  250  252\n",
      "  253  254  255  256  257  258  259  260  262  264  265  266  267  268\n",
      "  269  270  271  272  274  275  276  277  278  279  280  281  283  284\n",
      "  286  287  288  289  290  291  292  294  295  297  298  299  300  301\n",
      "  302  303  304  305  306  307  309  310  311  313  314  316  317  318\n",
      "  320  321  322  323  324  326  328  329  330  331  333  334  335  336\n",
      "  337  338  339  340  341  342  343  344  345  346  347  348  349  350\n",
      "  351  352  353  354  355  356  357  359  360  361  362  363  364  365\n",
      "  366  367  368  369  371  373  374  375  377  379  380  381  382  384\n",
      "  385  386  387  388  389  391  392  393  394  395  396  397  398  399\n",
      "  400  401  402  403  404  405  406  407  408  409  410  411  412  413\n",
      "  414  415  416  418  421  422  423  424  425  426  427  428  429  430\n",
      "  431  432  433  434  435  436  437  438  439  440  441  443  444  445\n",
      "  446  447  448  449  451  452  453  454  456  457  458  459  460  461\n",
      "  462  463  464  466  467  469  471  472  473  476  477  478  479  480\n",
      "  481  482  484  487  489  492  493  494  495  496  497  498  499  500\n",
      "  502  503  504  505  506  507  508  509  510  513  514  515  517  518\n",
      "  520  522  525  526  527  528  529  530  531  532  533  534  536  537\n",
      "  538  539  540  541  542  543  545  547  548  549  550  551  552  553\n",
      "  554  555  556  559  560  561  562  563  564  565  566  567  569  570\n",
      "  571  572  573  574  575  576  577  579  581  582  583  585  587  588\n",
      "  589  590  591  592  593  594  595  596  597  602  603  605  606  607\n",
      "  608  609  610  611  612  613  614  615  616  617  621  622  624  626\n",
      "  627  628  629  630  631  632  633  635  636  637  639  640  641  642\n",
      "  643  644  645  646  647  648  649  650  651  652  653  654  655  656\n",
      "  658  659  660  661  662  663  664  665  667  669  670  671  672  673\n",
      "  674  675  676  677  678  679  682  683  685  686  687  689  690  691\n",
      "  692  693  694  695  696  698  699  700  701  702  703  704  705  706\n",
      "  708  710  711  712  714  716  718  719  720  722  723  724  725  726\n",
      "  727  728  729  730  731  732  734  735  736  739  740  741  743  744\n",
      "  745  747  748  749  750  751  753  754  755  756  757  758  759  760\n",
      "  761  762  765  768  770  771  772  773  774  775  777  778  779  780\n",
      "  781  782  783  785  786  787  788  789  790  792  793  794  795  796\n",
      "  798  799  800  801  802  804  805  806  807  808  809  810  811  812\n",
      "  813  814  815  816  817  818  819  820  821  822  823  824  825  826\n",
      "  827  828  830  833  836  840  841  842  843  844  845  846  847  848\n",
      "  849  850  851  853  854  855  857  858  860  861  863  864  865  866\n",
      "  867  868  869  870  871  872  873  874  875  876  877  878  879  880\n",
      "  882  883  884  885  886  887  888  889  890  891  892  893  894  895\n",
      "  896  897  898  899  900  901  902  903  905  906  907  908  909  910\n",
      "  911  912  913  915  916  917  918  919  920  922  923  924  925  926\n",
      "  929  930  931  932  933  934  935  936  938  939  940  941  942  943\n",
      "  944  945  946  948  949  950  951  952  953  954  955  956  957  958\n",
      "  959  960  961  962  963  965  966  967  969  970  971  972  973  974\n",
      "  975  977  978  979  980  981  982  984  985  986  987  988  989  991\n",
      "  992  993  994  995  996  998  999 1000 1001 1002 1004 1005 1006 1007\n",
      " 1008 1009 1010 1011 1012 1013 1014 1015 1017 1018 1020 1021 1022 1023\n",
      " 1024] TEST: [  10   13   14   17   22   25   26   31   37   43   47   51   70   82\n",
      "   84   86   90  101  103  104  119  126  132  133  136  137  142  153\n",
      "  154  168  170  188  203  204  207  211  218  226  235  236  238  241\n",
      "  251  261  263  273  282  285  293  296  308  312  315  319  325  327\n",
      "  332  358  370  372  376  378  383  390  417  419  420  442  450  455\n",
      "  465  468  470  474  475  483  485  486  488  490  491  501  511  512\n",
      "  516  519  521  523  524  535  544  546  557  558  568  578  580  584\n",
      "  586  598  599  600  601  604  618  619  620  623  625  634  638  657\n",
      "  666  668  680  681  684  688  697  707  709  713  715  717  721  733\n",
      "  737  738  742  746  752  763  764  766  767  769  776  784  791  797\n",
      "  803  829  831  832  834  835  837  838  839  852  856  859  862  881\n",
      "  904  914  921  927  928  937  947  964  968  976  983  990  997 1003\n",
      " 1016 1019]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.81818182 0.84615385 0.83216783 0.84507042 0.82394366 0.85915493]\n",
      "83.74454184313339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "for train_index,test_index in skf.split(x,y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    scores = cross_val_score(lr, x_train,y_train, cv=skf)\n",
    "    print(scores)\n",
    "    print(np.mean(scores)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea98c43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.81818182 0.84615385 0.83216783 0.84507042 0.82394366 0.85915493]\n",
      "83.74454184313339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression()\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(lr, x_train,y_train, cv=skf)\n",
    "print(scores)\n",
    "print(np.mean(scores)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85e1bdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 0 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0\n",
      " 0 1 0 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1\n",
      " 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1\n",
      " 1 1 1 1 0 0 0 1 1 1 0 0 1 1 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0\n",
      " 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "y1_pred=cross_val_predict(lr,x_test,y_test,cv=skf)\n",
    "print(y1_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f25748f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ee8dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68ba245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7ea53a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a52c3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
